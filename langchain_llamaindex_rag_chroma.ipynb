{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOABxTviIro7bWsCgb41thh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "86e0b23b919e4216bc162e4e22f8245e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9c4bf79574344f2bacfbc1413040c044",
              "IPY_MODEL_ec845b1d5a594464b7c91e5baea07385",
              "IPY_MODEL_56b1f4b14509453da920a2e9237d9a00"
            ],
            "layout": "IPY_MODEL_6230d9061c2b43e680d499aee1a8951e"
          }
        },
        "9c4bf79574344f2bacfbc1413040c044": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da11c47825174a25b073ec430cdbd997",
            "placeholder": "​",
            "style": "IPY_MODEL_c91210696afd4e808ae80f41c0f76d0c",
            "value": "Parsing nodes: 100%"
          }
        },
        "ec845b1d5a594464b7c91e5baea07385": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bed4a1e8a4d54f1d8e9ee75953a7dfe1",
            "max": 29,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d785312fa66e4c839d08d880b747f06b",
            "value": 29
          }
        },
        "56b1f4b14509453da920a2e9237d9a00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4fdaf36804b94a889bbc6d3cbba230ad",
            "placeholder": "​",
            "style": "IPY_MODEL_768a7c6f2882411daa3d8904fa9760c1",
            "value": " 29/29 [00:00&lt;00:00, 109.13it/s]"
          }
        },
        "6230d9061c2b43e680d499aee1a8951e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da11c47825174a25b073ec430cdbd997": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c91210696afd4e808ae80f41c0f76d0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bed4a1e8a4d54f1d8e9ee75953a7dfe1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d785312fa66e4c839d08d880b747f06b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4fdaf36804b94a889bbc6d3cbba230ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "768a7c6f2882411daa3d8904fa9760c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b85fd49951604b28a3ed56a62d041d44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d983748325ba445db18411c5ca96fc7e",
              "IPY_MODEL_bdf60a26b1bf4a5ab081ad123ac2f3cc",
              "IPY_MODEL_79ae718714184cd18cec92b2ac9142bf"
            ],
            "layout": "IPY_MODEL_88c43e5e175142eb9b4fea5553991e11"
          }
        },
        "d983748325ba445db18411c5ca96fc7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0056d36ca0b4401a9c7661440789a42",
            "placeholder": "​",
            "style": "IPY_MODEL_a34e480186c045e6a628ff865b713f33",
            "value": "Generating embeddings: 100%"
          }
        },
        "bdf60a26b1bf4a5ab081ad123ac2f3cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2652efc27a154ef39d8399545f841213",
            "max": 34,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c6d6979594bd4cb0ae469cd7473ce8f5",
            "value": 34
          }
        },
        "79ae718714184cd18cec92b2ac9142bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d731acef8f5f4a2e9ea8a050f1a251c1",
            "placeholder": "​",
            "style": "IPY_MODEL_4257fb69eb584294b74d28e56fe5cc7d",
            "value": " 34/34 [00:02&lt;00:00, 13.30it/s]"
          }
        },
        "88c43e5e175142eb9b4fea5553991e11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0056d36ca0b4401a9c7661440789a42": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a34e480186c045e6a628ff865b713f33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2652efc27a154ef39d8399545f841213": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6d6979594bd4cb0ae469cd7473ce8f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d731acef8f5f4a2e9ea8a050f1a251c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4257fb69eb584294b74d28e56fe5cc7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/estellacoding/ll-rag-chroma/blob/main/langchain_llamaindex_rag_chroma.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain RAG"
      ],
      "metadata": {
        "id": "Q7RwCHFjynb-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 安裝套件"
      ],
      "metadata": {
        "id": "EzZYX0-nULBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf\n",
        "!pip install langchain_community\n",
        "!pip install langchain-openai\n",
        "!pip install langchain-chroma\n",
        "!pip install chromadb\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "0gpZ6r8RO5iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 下載資料"
      ],
      "metadata": {
        "id": "TDsljl3lOzUL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jHgsj0CQ4JVw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ffd6674f-0668-4eaa-eeb8-f18b49e02709"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-15 02:09:08--  https://openreview.net/pdf?id=VtmBAGCN7o\n",
            "Resolving openreview.net (openreview.net)... 35.184.86.251\n",
            "Connecting to openreview.net (openreview.net)|35.184.86.251|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16911937 (16M) [application/pdf]\n",
            "Saving to: ‘data/metagpt.pdf’\n",
            "\n",
            "data/metagpt.pdf    100%[===================>]  16.13M  7.73MB/s    in 2.1s    \n",
            "\n",
            "2025-01-15 02:09:11 (7.73 MB/s) - ‘data/metagpt.pdf’ saved [16911937/16911937]\n",
            "\n",
            "data  sample_data\n"
          ]
        }
      ],
      "source": [
        "!mkdir \"data\"\n",
        "!wget \"https://openreview.net/pdf?id=VtmBAGCN7o\" -O data/metagpt.pdf\n",
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 載入資料"
      ],
      "metadata": {
        "id": "_aonlUoFPTl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "loader = PyPDFLoader(\"./data/metagpt.pdf\")\n",
        "# 設定每片段長度為1000字元，重疊200字元\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=200)\n",
        "pages = loader.load_and_split(text_splitter=text_splitter)\n",
        "\n",
        "print(pages[-1].page_content)"
      ],
      "metadata": {
        "id": "qjJcwz4ZNh6D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c75fa506-ef27-4238-bf7b-50cc32fbb20c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8 5.00 215.00 43.00 3.00 301.00 100.33 29372.00 6499.00 621.73 $ 1.27 1. tensorflow ver-\n",
            "sion error 2. model\n",
            "training method not\n",
            "implement\n",
            "2\n",
            "9 5.00 215.00 43.00 3.00 270.00 90.00 24799.00 5734.00 550.88 $ 1.27 1. dependency er-\n",
            "ror 2. URL 403 er-\n",
            "ror\n",
            "3\n",
            "10 3.00 93.00 31.00 3.00 254.00 84.67 24109.00 5363.00 438.50 $ 0.92 1. dependency er-\n",
            "ror 2. missing main\n",
            "func.\n",
            "4\n",
            "Avg. 4.71 191.57 42.98 3.00 240.00 80.00 26626.86 6218.00 516.71 $1.12 0.51 (only consider\n",
            "item scored 2, 3 or\n",
            "4)\n",
            "3.36\n",
            "29\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 向量化"
      ],
      "metadata": {
        "id": "LoHxUMONT0Mv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 若資料夾存在則刪除\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "if os.path.exists(\"./vector\"):\n",
        "    shutil.rmtree(\"./vector\")"
      ],
      "metadata": {
        "id": "vYefkNdIPl2a"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 文本 -> 分割/索引 -> 向量化 -> 建立資料庫 -> 儲存\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# 使用OpenAI的嵌入模型將文本轉換為嵌入向量\n",
        "embeddings = OpenAIEmbeddings(api_key=OPENAI_API_KEY)\n",
        "\n",
        "# 創建向量索引\n",
        "index_creator = VectorstoreIndexCreator(\n",
        "    embedding=embeddings,\n",
        "    vectorstore_cls=Chroma, # 設定向量資料庫\n",
        "    vectorstore_kwargs={\"persist_directory\": \"./vector\"}\n",
        ")\n",
        "\n",
        "# 從分割文檔(pages)進行文本分割、向量化及索引並儲存\n",
        "docsearch = index_creator.from_documents(pages)"
      ],
      "metadata": {
        "id": "asF09L1fWYLo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 向量資料庫"
      ],
      "metadata": {
        "id": "GM-37HUYURRv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "\n",
        "db = Chroma(embedding_function=embeddings, persist_directory='./vector')"
      ],
      "metadata": {
        "id": "JI6EIDKMm15Q"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_context = db.similarity_search(\"Describe the five roles in MetaGPT framework\", k=10)\n",
        "for doc in similarity_context:\n",
        "    print(\"-\"*66)\n",
        "    print(doc.page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cESnqvSkPAQ",
        "outputId": "1e70f2fb-6c7e-431f-ced1-d71c0b2afb05"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------\n",
            "MetaGPT is a meta-programming framework for LLM-based multi-agent systems. Sec. 3.1 pro-\n",
            "vides an explanation of role specialization, workflow and structured communication in this frame-\n",
            "work, and illustrates how to organize a multi-agent system within the context of SOPs. Sec. 3.2\n",
            "presents a communication protocol that enhances role communication efficiency. We also imple-\n",
            "ment structured communication interfaces and an effective publish-subscribe mechanism. These\n",
            "------------------------------------------------------------------\n",
            "agents within the MetaGPT framework. This platform provides users with an operational interface,\n",
            "allowing users to easily manage a variety of agents with different emotions, personalities, and capa-\n",
            "bilities for specific tasks.\n",
            "16\n",
            "------------------------------------------------------------------\n",
            "Preprint\n",
            "• We introduce MetaGPT, a meta-programming framework for multi-agent collaboration based on\n",
            "LLMs. It is highly convenient and flexible, with well-defined functions like role definition and\n",
            "message sharing, making it a useful platform for developing LLM-based multi-agent systems.\n",
            "• Our innovative integration of human-like SOPs throughout MetaGPT’s design significantly en-\n",
            "hances its robustness, reducing unproductive collaboration among LLM-based agents. Furthermore,\n",
            "------------------------------------------------------------------\n",
            "titative results of MetaGPT and MetaGPT without executable feedback are shown in Table 4 and\n",
            "Table 9.\n",
            "5 C ONCLUSION\n",
            "This work introduces MetaGPT, a novel meta-programming framework that leverages SOPs to en-\n",
            "hance the problem-solving capabilities of multi-agent systems based on Large Language Models\n",
            "(LLMs). MetaGPT models a group of agents as a simulated software company, analogous to simu-\n",
            "lated towns (Park et al., 2023) and the Minecraft Sandbox in V oyager (Wang et al., 2023a). MetaGPT\n",
            "------------------------------------------------------------------\n",
            "(LLMs). MetaGPT models a group of agents as a simulated software company, analogous to simu-\n",
            "lated towns (Park et al., 2023) and the Minecraft Sandbox in V oyager (Wang et al., 2023a). MetaGPT\n",
            "leverages role specialization, workflow management, and efficient sharing mechanisms such as mes-\n",
            "sage pools and subscriptions, rendering it a flexible and portable platform for autonomous agents\n",
            "and multi-agent frameworks. It uses an executable feedback mechanism to enhance code generation\n",
            "------------------------------------------------------------------\n",
            "insights, while a software engineer is responsible for programming. We define five roles in our\n",
            "software company: Product Manager, Architect, Project Manager, Engineer, and QA Engineer, as\n",
            "shown in Figure 1. In MetaGPT, we specify the agent’s profile, which includes their name, profile,\n",
            "goal, and constraints for each role. We also initialize the specific context and skills for each role.\n",
            "For instance, a Product Manager can use web search tools, while an Engineer can execute code, as\n",
            "------------------------------------------------------------------\n",
            "et al., 2023). However, MetaGPT stands out as a unique solution that allows for efficient meta-\n",
            "programming through a well-organized group of specialized agents. Each agent has a specific role\n",
            "and expertise, following some established standards. This allows for automatic requirement analysis,\n",
            "system design, code generation, modification, execution, and debugging during runtime, highlight-\n",
            "ing how agent-based techniques can enhance meta-programming.\n",
            "------------------------------------------------------------------\n",
            "streamlined workflows, thus allowing agents with human-like domain expertise\n",
            "to verify intermediate results and reduce errors. MetaGPT utilizes an assembly\n",
            "line paradigm to assign diverse roles to various agents, efficiently breaking down\n",
            "complex tasks into subtasks involving many agents working together. On col-\n",
            "laborative software engineering benchmarks, MetaGPT generates more coherent\n",
            "solutions than previous chat-based multi-agent systems. Our project can be found\n",
            "------------------------------------------------------------------\n",
            "AgentVerse and ChatDev, MetaGPT offers functions for software engineering tasks. As presented\n",
            "in Table 2, our framework encompasses a wide range of abilities to handle complex and specialized\n",
            "development tasks efficiently. Incorporating SOPs (e.g., role-play expertise, structured communi-\n",
            "cation, streamlined workflow) can significantly improve code generation. Other baseline methods\n",
            "8\n",
            "------------------------------------------------------------------\n",
            "for programming-related positions. Furthermore, programming with natural language may offer a\n",
            "significantly easier learning curve, making programming more accessible to a broader audience.\n",
            "Transparency and Accountability MetaGPT is an open-source framework that facilitates inter-\n",
            "active communication between multiple agents through natural language. Humans can initiate, ob-\n",
            "serve, and stop running with the highest level of control. It provides real-time interpretation and op-\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 開啟查詢"
      ],
      "metadata": {
        "id": "GSNpaG2jUUaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain.chains import create_retrieval_chain\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o-mini\", openai_api_key=OPENAI_API_KEY)\n",
        "\n",
        "# 設置檢索器:檢索最相關的k段內容\n",
        "retriever = db.as_retriever(search_kwargs={\"k\": 100})\n",
        "\n",
        "system_prompt = (\n",
        "    \"你是一個專業的助理，請從給定內容中提取準確答案。\"\n",
        "    \"Context: {context}\"\n",
        ")\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_prompt),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 創建處理文檔的鏈條\n",
        "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
        "\n",
        "# 創建檢索鏈條\n",
        "chain = create_retrieval_chain(retriever, question_answer_chain)"
      ],
      "metadata": {
        "id": "IpWk_Jpjedf6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = chain.invoke({\"input\": \"What are the five roles in the MetaGPT framework?\"})\n",
        "result['answer']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "KXrFgN-XCnuh",
        "outputId": "e092606a-c67e-4071-b80a-ac50c6de6333"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The five roles in the MetaGPT framework are: Product Manager, Architect, Project Manager, Engineer, and QA Engineer.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = chain.invoke({\"input\": \"Describe the five roles in MetaGPT framework\"})\n",
        "result['answer']"
      ],
      "metadata": {
        "id": "D9Lof1-BVygM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "outputId": "2bc2aac4-1ac2-4277-a4ae-7bdebb53c166"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'In the MetaGPT framework, the five roles defined are:\\n\\n1. **Product Manager**: Responsible for analyzing competition and user needs to create Product Requirements Documents (PRDs) that guide the developmental process.\\n\\n2. **Architect**: Focuses on system design, generating system interface designs and flow diagrams based on the PRDs provided by the Product Manager.\\n\\n3. **Project Manager**: Manages the overall project workflow and ensures that tasks are distributed effectively among team members.\\n\\n4. **Engineer**: Executes code based on the system design and PRDs, and is responsible for coding tasks and implementing functionalities.\\n\\n5. **QA Engineer**: Formulates test cases to ensure code quality and performs quality assurance checks on the produced software to validate its functionality and reliability.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = chain.invoke({\"input\": \"說明 MetaGPT 框架中的五個角色\"})\n",
        "result['answer']"
      ],
      "metadata": {
        "id": "kaKtnIQvacqN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "3cbf67c9-3dba-478d-ab28-1c38a66ace3c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'MetaGPT 框架中的五個角色包括：\\n\\n1. **產品經理 (Product Manager)**：負責生成產品需求文檔 (PRD)，分析市場競爭和用戶需求，以指導開發過程。\\n\\n2. **架構師 (Architect)**：負責系統界面的設計，生成系統模塊設計和交互序列的文檔，確保系統的整體架構符合需求。\\n\\n3. **項目經理 (Project Manager)**：負責任務分配，協調各角色之間的合作，確保項目的順利進行。\\n\\n4. **工程師 (Engineer)**：負責根據設計文檔執行代碼，編寫實際的軟件解決方案，並進行測試和調試。\\n\\n5. **質量保證工程師 (QA Engineer)**：負責制定測試用例，檢查代碼的質量，確保最終產品符合規範和需求。 \\n\\n這些角色協同工作，通過標準化操作程序 (SOPs) 提升軟件開發的效率和質量。'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Llamaindex RAG"
      ],
      "metadata": {
        "id": "h-Y4ncqV1rZ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 安裝套件"
      ],
      "metadata": {
        "id": "WOel5O4E1zmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama_index\n",
        "!pip install llama-index-embeddings-openai\n",
        "!pip install llama-index-vector-stores-chroma"
      ],
      "metadata": {
        "id": "y3HOfo0j1xgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 下載資料"
      ],
      "metadata": {
        "id": "27KxTdKZ11s2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir \"data\"\n",
        "!curl -L \"https://openreview.net/pdf?id=VtmBAGCN7o\" -e \"https://openreview.net/pdf?id=VtmBAGCN7o\" -o \"data/metagpt.pdf\"\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qefxOnn13LL",
        "outputId": "35f97820-ac54-4216-c5f7-edad0bebfb04"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 16.1M  100 16.1M    0     0  20.1M      0 --:--:-- --:--:-- --:--:-- 20.1M\n",
            "data  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 載入資料"
      ],
      "metadata": {
        "id": "8izrJKry1-83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "\n",
        "# 讀取指定資料夾內的所有檔案\n",
        "documents = SimpleDirectoryReader(\"./data\").load_data(show_progress=True)\n",
        "print(\"載入的文件列表:\", documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kaakNtf2IkN",
        "outputId": "6e50ce4c-10fb-4892-df86-4dec80bd3abf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading files: 100%|██████████| 1/1 [00:03<00:00,  3.51s/file]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "載入的文件列表: [Document(id_='4500a20b-3619-41d8-8a33-d982cdf5a3a4', embedding=None, metadata={'page_label': '1', 'file_name': 'metagpt.pdf', 'file_path': '/content/data/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-01-15', 'last_modified_date': '2025-01-15'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Preprint\\nMETAGPT: M ETA PROGRAMMING FOR A\\nMULTI -AGENT COLLABORATIVE FRAMEWORK\\nSirui Hong1∗, Mingchen Zhuge2∗, Jonathan Chen1, Xiawu Zheng3, Yuheng Cheng4,\\nCeyao Zhang4, Jinlin Wang1, Zili Wang, Steven Ka Shing Yau5, Zijuan Lin4,\\nLiyang Zhou6, Chenyu Ran1, Lingfeng Xiao1,7, Chenglin Wu1†, J¨urgen Schmidhuber2,8\\n1DeepWisdom, 2AI Initiative, King Abdullah University of Science and Technology,\\n3Xiamen University, 4The Chinese University of Hong Kong, Shenzhen,\\n5Nanjing University, 6University of Pennsylvania,\\n7University of California, Berkeley, 8The Swiss AI Lab IDSIA/USI/SUPSI\\nABSTRACT\\nRemarkable progress has been made on automated problem solving through so-\\ncieties of agents based on large language models (LLMs). Existing LLM-based\\nmulti-agent systems can already solve simple dialogue tasks. Solutions to more\\ncomplex tasks, however, are complicated through logic inconsistencies due to\\ncascading hallucinations caused by naively chaining LLMs. Here we introduce\\nMetaGPT, an innovative meta-programming framework incorporating efficient\\nhuman workflows into LLM-based multi-agent collaborations. MetaGPT en-\\ncodes Standardized Operating Procedures (SOPs) into prompt sequences for more\\nstreamlined workflows, thus allowing agents with human-like domain expertise\\nto verify intermediate results and reduce errors. MetaGPT utilizes an assembly\\nline paradigm to assign diverse roles to various agents, efficiently breaking down\\ncomplex tasks into subtasks involving many agents working together. On col-\\nlaborative software engineering benchmarks, MetaGPT generates more coherent\\nsolutions than previous chat-based multi-agent systems. Our project can be found\\nat https://github.com/geekan/MetaGPT.\\n1 I NTRODUCTION\\nAutonomous agents utilizing Large Language Models (LLMs) offer promising opportunities to en-\\nhance and replicate human workflows. In real-world applications, however, existing systems (Park\\net al., 2023; Zhuge et al., 2023; Cai et al., 2023; Wang et al., 2023c; Li et al., 2023; Du et al., 2023;\\nLiang et al., 2023; Hao et al., 2023) tend to oversimplify the complexities. They struggle to achieve\\neffective, coherent, and accurate problem-solving processes, particularly when there is a need for\\nmeaningful collaborative interaction (Chen et al., 2024; Zhang et al., 2023; Dong et al., 2023; Zhou\\net al., 2023; Qian et al., 2023).\\nThrough extensive collaborative practice, humans have developed widely accepted Standardized\\nOperating Procedures (SOPs) across various domains (Belbin, 2012; Manifesto, 2001; DeMarco &\\nLister, 2013). These SOPs play a critical role in supporting task decomposition and effective coor-\\ndination. Furthermore, SOPs outline the responsibilities of each team member, while establishing\\nstandards for intermediate outputs. Well-defined SOPs improve the consistent and accurate exe-\\ncution of tasks that align with defined roles and quality standards (Belbin, 2012; Manifesto, 2001;\\nDeMarco & Lister, 2013; Wooldridge & Jennings, 1998). For instance, in a software company,\\nProduct Managers analyze competition and user needs to create Product Requirements Documents\\n(PRDs) using a standardized structure, to guide the developmental process.\\nInspired by such ideas, we design a promising GPT -based Meta-Programming framework called\\nMetaGPT that significantly benefits from SOPs. Unlike other works (Li et al., 2023; Qian et al.,\\n2023), MetaGPT requires agents to generate structured outputs, such as high-quality requirements\\n∗These authors contributed equally to this work.\\n†Chenglin Wu (alexanderwu@fuzhi.ai) is the corresponding author, affiliated with DeepWisdom.\\n1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='af65feeb-684c-47cc-8a8e-3bdb8cfd52e9', embedding=None, metadata={'page_label': '2', 'file_name': 'metagpt.pdf', 'file_path': '/content/data/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-01-15', 'last_modified_date': '2025-01-15'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Preprint\\nFigure 1: The software development SOPs between MetaGPT and real-world human teams.\\nIn software engineering, SOPs promote collaboration among various roles. MetaGPT showcases\\nits ability to decompose complex tasks into specific actionable procedures assigned to various roles\\n(e.g., Product Manager, Architect, Engineer, etc.).\\ndocuments, design artifacts, flowcharts, and interface specifications. The use of intermediate struc-\\ntured outputs significantly increases the success rate of target code generation. Because it helps\\nmaintain consistency in communication, minimizing ambiguities and errors during collaboration.\\nMore graphically, in a company simulated by MetaGPT, all employees follow a strict and stream-\\nlined workflow, and all their handovers must comply with certain established standards. This reduces\\nthe risk of hallucinations caused by idle chatter between LLMs, particularly in role-playing frame-\\nworks, like: “ Hi, hello and how are you?” – Alice (Product Manager); “ Great! Have you had\\nlunch?” – Bob (Architect).\\nBenefiting from SOPs, MetaGPT offers a promising approach to meta-programming. In this context,\\nwe adopt meta-programming1 as ”programming to program”, in contrast to the broader fields of meta\\nlearning and ”learning to learn” (Schmidhuber, 1987; 1993a; Hochreiter et al., 2001; Schmidhuber,\\n2006; Finn et al., 2017).\\nThis notion of meta-programming also encompasses earlier efforts like CodeBERT (Feng et al.,\\n2020) and recent projects such as CodeLlama (Rozi `ere et al., 2023) and WizardCoder (Luo\\net al., 2023). However, MetaGPT stands out as a unique solution that allows for efficient meta-\\nprogramming through a well-organized group of specialized agents. Each agent has a specific role\\nand expertise, following some established standards. This allows for automatic requirement analysis,\\nsystem design, code generation, modification, execution, and debugging during runtime, highlight-\\ning how agent-based techniques can enhance meta-programming.\\nTo validate the design of MetaGPT, we use publicly available HumanEval (Chen et al., 2021a) and\\nMBPP (Austin et al., 2021) for evaluations. Notably, in code generation benchmarks, MetaGPT\\nachieves a new state-of-the-art (SoTA) with 85.9% and 87.7% in Pass@1. When compared to other\\npopular frameworks for creating complex software projects, such as AutoGPT (Torantulino et al.,\\n2023), LangChain (Chase, 2022), AgentVerse (Chen et al., 2023), and ChatDev (Qian et al., 2023).\\nMetaGPT also stands out in handling higher levels of software complexity and offering extensive\\nfunctionality. Remarkably, in our experimental evaluations, MetaGPT achieves a 100% task com-\\npletion rate, demonstrating the robustness and efficiency (time and token costs) of our design.\\nWe summarize our contributions as follows:\\n1https://en.wikipedia.org/w/index.php?title=Metaprogramming\\n2', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='c32ff855-5780-4d04-b5bd-f59dc2890dd3', embedding=None, metadata={'page_label': '3', 'file_name': 'metagpt.pdf', 'file_path': '/content/data/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-01-15', 'last_modified_date': '2025-01-15'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Preprint\\n• We introduce MetaGPT, a meta-programming framework for multi-agent collaboration based on\\nLLMs. It is highly convenient and flexible, with well-defined functions like role definition and\\nmessage sharing, making it a useful platform for developing LLM-based multi-agent systems.\\n• Our innovative integration of human-like SOPs throughout MetaGPT’s design significantly en-\\nhances its robustness, reducing unproductive collaboration among LLM-based agents. Furthermore,\\nwe introduce a novel executive feedback mechanism that debugs and executes code during runtime,\\nsignificantly elevating code generation quality (e.g., 5.4% absolute improvement on MBPP).\\n• We achieve state-of-the-art performance on HumanEval (Chen et al., 2021a) and MBPP (Austin\\net al., 2021). Extensive results convincingly validate MetaGPT, suggesting that it is a promising\\nmeta-programming framework for developing LLM-based multi-agent systems.\\n2 R ELATED WORK\\nAutomatic Programming The roots of automatic programming reach back deep into the previ-\\nous century. In 1969, Waldinger & Lee (1969) introduced “PROW,” a system designed to accept\\nprogram specifications written in predicate calculus, generate algorithms, and create LISP imple-\\nmentations (McCarthy, 1978). Balzer (1985) and Soloway (1986) made efforts to advance auto-\\nmatic programming and identified potential methods to achieve it. Recent approaches use natural\\nlanguage processing (NLP) techniques (Ni et al., 2023; Skreta et al., 2023; Feng et al., 2020; Li\\net al., 2022; Chen et al., 2018; 2021b; Zhang et al., 2023). Automatic programming has grown into\\nan industry delivering paid functions such as Microsoft Copilot. Lately, LLMs-based agents (Yao\\net al., 2022; Shinn et al., 2023; Lin et al., 2023) have advanced automatic programming develop-\\nment. Among them, ReAct (Yao et al., 2022) and Reflexion (Shinn et al., 2023) utilize a chain of\\nthought prompts (Wei et al., 2022) to generate reasoning trajectories and action plans with LLMs.\\nBoth works demonstrate the effectiveness of the ReAct style loop of reasoning as a design paradigm\\nfor empowering automatic programming. Additionally, ToolFormer (Schick et al., 2023) can learn\\nhow to use external tools through simple APIs. The research most closely aligned with our work\\nby Li et al. (2023) proposes a straightforward role-play framework for programming that involves\\ncommunication between agents playing different roles. Qian et al. (2023) utilizes multiple agents for\\nsoftware development. Although existing papers (Li et al., 2023; Qian et al., 2023) have improved\\nproductivity, they have not fully tapped into effective workflows with structured output formats.\\nThis makes it harder to deal with complex software engineering issues.\\nLLM-Based Multi-Agent Frameworks Recently, LLM-based autonomous agents have gained\\ntremendous interest in both industry and academia (Wang et al., 2023b). Many works (Chen et al.,\\n2024; Wang et al., 2023c; Du et al., 2023; Zhuge et al., 2023; Hao et al., 2023; Akata et al., 2023)\\nhave improved the problem-solving abilities of LLMs by integrating discussions among multiple\\nagents. Stable-Alignment (Liu et al., 2023) creates instruction datasets by deriving consensus on\\nvalue judgments through interactions across a sandbox with LLM agents. Other works focus on\\nsociological phenomena. For example, Generative Agents (Park et al., 2023) creates a “town” of 25\\nagents to study language interaction, social understanding, and collective memory. In the Natural\\nLanguage-Based Society of Mind (NLSOM) (Zhuge et al., 2023), agents with different functions\\ninteract to solve complex tasks through multiple rounds of “mindstorms.” Cai et al. (2023) propose\\na model for cost reduction by combining large models as tool makers and small models as tool users.\\nSome works emphasize cooperation and competition related to planning and strategy (Bakhtin et al.,\\n2022); others propose LLM-based economies (Zhuge et al., 2023). These works focus on open-\\nworld human behavior simulation, while MetaGPT aims to introduce human practice into multi-\\nagents frameworks. Besides, LLM-based agents face the challenges of “assistant repeated instruc-\\ntion” or “infinite loop of message” (Talebirad & Nadiri, 2023; Li et al., 2023). These challenges\\nbecome more urgent in task-oriented collaborations, which require consistent and mutually benefi-\\ncial interactions (Elazar et al., 2021; Wang et al., 2022; Jiang et al., 2023). This motivates our focus\\non applying advanced concepts such as Standard Operating Procedures in software development to\\nmulti-agent frameworks.\\n3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='c1565f62-850f-4d0a-987b-069c66fdc17f', embedding=None, metadata={'page_label': '4', 'file_name': 'metagpt.pdf', 'file_path': '/content/data/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-01-15', 'last_modified_date': '2025-01-15'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Preprint\\nFigure 2: An example of the communication protocol (left) and iterative programming with exe-\\ncutable feedback (right). Left: Agents use a shared message pool to publish structured messages.\\nThey can also subscribe to relevant messages based on their profiles. Right: After generating the\\ninitial code, the Engineer agent runs and checks for errors. If errors occur, the agent checks past\\nmessages stored in memory and compares them with the PRD, system design, and code files.\\n3 M ETAGPT: A M ETA-PROGRAMMING FRAMEWORK\\nMetaGPT is a meta-programming framework for LLM-based multi-agent systems. Sec. 3.1 pro-\\nvides an explanation of role specialization, workflow and structured communication in this frame-\\nwork, and illustrates how to organize a multi-agent system within the context of SOPs. Sec. 3.2\\npresents a communication protocol that enhances role communication efficiency. We also imple-\\nment structured communication interfaces and an effective publish-subscribe mechanism. These\\nmethods enable agents to obtain directional information from other roles and public information\\nfrom the environment. Finally, we introduce executable feedback—a self-correction mechanism for\\nfurther enhancing code generation quality during run-time in Sec. 3.3.\\n3.1 A GENTS IN STANDARD OPERATING PROCEDURES\\nSpecialization of Roles Unambiguous role specialization enables the breakdown of complex work\\ninto smaller and more specific tasks. Solving complex tasks or problems often requires the collab-\\noration of agents with diverse skills and expertise, each contributing specialized outputs tailored to\\nspecific issues.\\nIn a software company, a Product Manager typically conducts business-oriented analysis and derives\\ninsights, while a software engineer is responsible for programming. We define five roles in our\\nsoftware company: Product Manager, Architect, Project Manager, Engineer, and QA Engineer, as\\nshown in Figure 1. In MetaGPT, we specify the agent’s profile, which includes their name, profile,\\ngoal, and constraints for each role. We also initialize the specific context and skills for each role.\\nFor instance, a Product Manager can use web search tools, while an Engineer can execute code, as\\nshown in Figure 2. All agents adhere to the React-style behavior as described in Yao et al. (2022).\\nEvery agent monitors the environment ( i.e., the message pool in MetaGPT) to spot important ob-\\nservations (e.g.,, messages from other agents). These messages can either directly trigger actions or\\nassist in finishing the job.\\nWorkflow across Agents By defining the agents’ roles and operational skills, we can establish\\nbasic workflows. In our work, we follow SOP in software development, which enables all agents to\\nwork in a sequential manner.\\n4', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='259132a6-4f74-4026-b738-5e7e8e2aa034', embedding=None, metadata={'page_label': '5', 'file_name': 'metagpt.pdf', 'file_path': '/content/data/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-01-15', 'last_modified_date': '2025-01-15'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Preprint\\nFigure 3: A diagram showing the software development process in MetaGPT, emphasizing its sig-\\nnificant dependence on SOPs. The more detailed demonstration can be found in Appendix B.\\nSpecifically, as shown in Figure 1, upon obtaining user requirements, the Product Manager under-\\ntakes a thorough analysis, formulating a detailed PRD that includes User Stories and Requirement\\nPool. This serves as a preliminary functional breakdown. The structured PRD is then passed to\\nthe Architect, who translates the requirements into system design components, such as File Lists,\\nData Structures, and Interface Definitions. Once captured in the system design, the information is\\ndirected towards the Project Manager for task distribution. Engineers proceed to execute the des-\\nignated classes and functions as outlined (detailed in Figure 2). In the following stage, the QA\\nEngineer formulates test cases to enforce stringent code quality. In the final step, MetaGPT pro-\\nduces a meticulously crafted software solution. We provide a detailed schematic (Figure 3) and a\\nconcrete instance (Appendix B) of the SOP workflow in MetaGPT.\\n3.2 C OMMUNICATION PROTOCOL\\nStructured Communication Interfaces Most current LLM-based multi-agent frameworks (Li\\net al., 2023; Zhuge et al., 2023; Zhang et al., 2023; Park et al., 2023) utilize unconstrained natural\\nlanguage as a communication interface.\\nHowever, despite the versatility of natural language, a question arises: does pure natural language\\ncommunication suffice for solving complex tasks? For example, in the telephone game (or Chinese\\n5', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='2c42cdac-e3f6-4596-b209-b06c90faf649', embedding=None, metadata={'page_label': '6', 'file_name': 'metagpt.pdf', 'file_path': '/content/data/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-01-15', 'last_modified_date': '2025-01-15'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Preprint\\nwhispers)2, after several rounds of communication, the original information may be quite distorted.\\nInspired by human social structures, we propose using structured communication to formulate the\\ncommunication of agents. We establish a schema and format for each role and request that individ-\\nuals provide the necessary outputs based on their specific role and context.\\nAs shown in Figure 3, the Architect agent generates two outputs: the system interface design and a\\nsequence flow diagram. These contain system module design and interaction sequences, which serve\\nas important deliverables for Engineers. Unlike ChatDev (Zhao et al., 2023), agents in MetaGPT\\ncommunicate through documents and diagrams (structured outputs) rather than dialogue. These\\ndocuments contain all necessary information, preventing irrelevant or missing content.\\nPublish-Subscribe Mechanism Sharing information is critical in collaboration. For instance,\\nArchitects and Engineers often need to reference PRDs. However, communicating this information\\neach time in a one-to-one manner, as indicated by previous work (Li et al., 2023; Zhao et al., 2023;\\nZhang et al., 2023), can complicate the communication topology, resulting in inefficiencies.\\nTo address this challenge, a viable approach is to store information in a global message pool. As\\nshown in Figure 2 (left), we introduce a shared message pool that allows all agents to exchange\\nmessages directly. These agents not onlypublish their structured messages in the pool but also access\\nmessages from other entities transparently. Any agent can directly retrieve required information\\nfrom the shared pool, eliminating the need to inquire about other agents and await their responses.\\nThis enhances communication efficiency.\\nSharing all information with every agent can lead to information overload. During task execution,\\nan agent typically prefers to receive only task-related information and avoid distractions through\\nirrelevant details. Effective management and dissemination of this information play a crucial role.\\nWe offer a simple and effective solution- subscription mechanism (in Figure 2 (left)). Instead of\\nrelying on dialogue, agents utilize role-specific interests to extract relevant information. They can\\nselect information to follow based on their role profiles. In practical implementations, an agent\\nactivates its action only after receiving all its prerequisite dependencies. As illustrated in Figure 3,\\nthe Architect mainly focuses on PRDs provided by the Product Manager, while documents from\\nroles such as the QA Engineer might be of lesser concern.\\n3.3 I TERATIVE PROGRAMMING WITH EXECUTABLE FEEDBACK\\nIn daily programming tasks, the processes of debugging and optimization play important roles.\\nHowever, existing methods often lack a self-correction mechanism, which leads to unsuccessful code\\ngeneration. Previous work introduced non-executable code review and self-reflection (Zhao et al.,\\n2023; Yao et al., 2022; Shinn et al., 2023; Dong et al., 2023). However, they still face challenges in\\nensuring code executability and runtime correctness.\\nOur first MetaGPT implementations overlooked certain errors during the review process, due to\\nLLM hallucinations (Manakul et al., 2023). To overcome this, after initial code generation, we\\nintroduce an executable feedback mechanism to improve the code iteratively. More specifically, as\\nshown in Figure 2, the Engineer is asked to write code based on the original product requirements\\nand design.\\nThis enables the Engineer to continuously improve code using its own historical execution and\\ndebugging memory. To obtain additional information, the Engineer writes and executes the corre-\\nsponding unit test cases, and subsequently receives the test results. If satisfactory, additional devel-\\nopment tasks are initiated. Otherwise the Engineer debugs the code before resuming programming.\\nThis iterative testing process continues until the test is passed or a maximum of 3 retries is reached.\\n4 E XPERIMENTS\\n4.1 E XPERIMENTAL SETTING\\nDatasets We use two public benchmarks, HumanEval (Chen et al., 2021a) and MBPP (Austin\\net al., 2021), and a self-generated, more challenging software development benchmark named Soft-\\n2https://en.wikipedia.org/wiki/Chinese whispers\\n6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='039b519b-68d6-4ea5-8a4e-3fb509342a2f', embedding=None, metadata={'page_label': '7', 'file_name': 'metagpt.pdf', 'file_path': '/content/data/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-01-15', 'last_modified_date': '2025-01-15'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Preprint\\nwareDev: (1) HumanEval includes 164 handwritten programming tasks. These tasks encompass\\nfunction specifications, descriptions, reference codes, and tests. (2) MBPP consists of 427 Python\\ntasks. These tasks cover core concepts and standard library features and include descriptions, ref-\\nerence codes, and automated tests. (3) Our SoftwareDev dataset is a collection of 70 representa-\\ntive examples of software development tasks, each with its own task prompt (see Table 8). These\\ntasks have diverse scopes (See Figure 5), such as mini-games, image processing algorithms, data\\nvisualization. They offer a robust testbed for authentic development tasks. Contrary to previous\\ndatasets (Chen et al., 2021a; Austin et al., 2021), SoftwareDev focuses on the engineering aspects.\\nIn the comparisons, we randomly select seven representative tasks for evaluation.\\nEvaluation Metrics For HuamnEval and MBPP, we follow the unbiased version of Pass @k as\\npresented by (Chen et al., 2021a; Dong et al., 2023), to evaluate the functional accuracy of the top-k\\ngenerated codes: Pass @k = EProblems\\n\\x14\\n1 − (\\nn−c\\nk )\\n(\\nn\\nk)\\n\\x15\\n.\\nFor SoftwareDev, we prioritize practical use and evaluate performance through human evaluations\\n(A, E) or statistical analysis (B, C, D): (A) Executability: this metric rates code from 1 (failure/non-\\nfunctional) to 4 (flawless). ‘1’ is for non-functional, ‘2’ for runnable but imperfect, ‘3’ for nearly\\nperfect, and ‘4’ for flawless code. (B) Cost: the cost evaluations here include the (1) running time,\\n(2) token usage, and (3) expenses. (C) Code Statistics: this includes (1) code files, (2) lines of code\\nper file, and (3) total code lines. (D) Productivity: basically, it is defined as the number of token\\nusage divided by the number of lines of code, which refers to the consumption of tokens per code\\nline. (E) Human Revision Cost: refers to times of manual code corrections, which tackle problems\\nlike package import errors, incorrect class names, or incomplete reference paths. Typically, each\\ncorrection involves up to 3 lines of code.\\nBaselines We compare our method with recent domain-specific LLMs in the code generation field,\\nincluding AlphaCode (Li et al., 2022), Incoder (Fried et al., 2022), CodeGeeX (Zheng et al., 2023),\\nCodeGen (Nijkamp et al., 2023), CodeX (Chen et al., 2021a), and CodeT (Chen et al., 2022) and\\ngeneral domain LLMs such as PaLM (Chowdhery et al., 2022), and GPT-4 (OpenAI, 2023). Several\\nresults of baselines (such as Incoder, CodeGeeX) are provided by Dong et al. (2023). In HumanEval\\nand MBPP, we slightly modified the prompts to align with response format requirements. These\\nmodifications aim to address format-specific issues (i.e., Python problems). With the SoftwareDev\\nbenchmark, we provide a comprehensive comparison between MetaGPT, AutoGPT (Torantulino\\net al., 2023), LangChain (Chase, 2022) with Python Read-Eval-Print Loop (REPL) tool 3, Agent-\\nVerse (Chen et al., 2023), and ChatDev (Qian et al., 2023).\\n4.2 M AIN RESULT\\nAlphaCode(1.1B)Incoder (6.7B)CodeGeeX (13B)\\n17.1 \\n— \\n15.2 17.6 18.9 26.9 \\nCodeGeeX-Mono(16.1B)\\n32.9 38.6 \\nGPT\\n-4\\n67.0 \\n— \\nMetaGPT\\n(w/o Feedback)\\n81.7 82.3 \\nPass@1 of MBPP and HumanEval (%)\\nPaLM Coder(540B)\\n36.0 47.0 \\nCodex (175B)\\n47.0 \\n58.1 \\nCodex + CodeT\\n65.8 67.7 \\nHumanEval\\nMBPP\\nMetaGPT\\n85.9 87.7 \\nFigure 4: Pass rates on the MBPP and HumanEval with a single attempt.\\nPerformance Figure 4 demonstrates that MetaGPT outperforms all preceding approaches in both\\nHumanEval and MBPP benchmarks. When MetaGPT collaborates with GPT-4, it significantly im-\\nproves the Pass @k in the HumanEval benchmark compared to GPT-4. It achieves 85.9% and 87.7%\\n3https://en.wikipedia.org/wiki/Read–eval–print loop\\n7', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='5e2221ef-7eb2-4908-990c-6597da7c6b7c', embedding=None, metadata={'page_label': '8', 'file_name': 'metagpt.pdf', 'file_path': '/content/data/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-01-15', 'last_modified_date': '2025-01-15'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Preprint\\nFigure 5: Demo softwares developed by MetaGPT.\\nin these two public benchmarks. Moreover, as shown in Table 1, MetaGPT outperforms ChatDev on\\nthe challenging SoftwareDev dataset in nearly all metrics. For example, considering the executabil-\\nity, MetaGPT achieves a score of 3.75, which is very close to 4 (flawless). Besides, it takes less time\\n(503 seconds), clearly less than ChatDev. Considering the code statistic and the cost of human revi-\\nsion, it also significantly outperforms ChatDev. Although MetaGPT requires more tokens (24,613\\nor 31,255 compared to 19,292), it needs only 126.5/124.3 tokens to generate one line of code. In\\ncontrast, ChatDev uses 248.9 tokens. These results highlight the benefits of SOPs in collabora-\\ntions between multiple agents. Additionally, we demonstrate the autonomous software generation\\ncapabilities of MetaGPT through visualization samples (Figure 5). For additional experiments and\\nanalysis, please refer to Appendix C.\\nTable 1: The statistical analysis on SoftwareDev.\\nStatistical Index ChatDev MetaGPT w/o Feedback MetaGPT\\n(A)Executability 2.25 3.67 3.75\\n(B)Cost#1: Running Times (s) 762 503 541\\n(B)Cost#2: Token Usage 19,292 24,613 31,255\\n(C)Code Statistic#1: Code Files 1.9 4.6 5.1\\n(C)Code Statistic#2: Lines of Code per File 40.8 42.3 49.3\\n(C)Code Statistic#3: Total Code Lines 77.5 194.6 251.4\\n(D)Productivity 248.9 126.5 124.3\\n(E)Human Revision Cost 2.5 2.25 0.83\\n4.3 C APABILITIES ANALYSIS\\nCompared to open-source baseline methods such as AutoGPT and autonomous agents such as\\nAgentVerse and ChatDev, MetaGPT offers functions for software engineering tasks. As presented\\nin Table 2, our framework encompasses a wide range of abilities to handle complex and specialized\\ndevelopment tasks efficiently. Incorporating SOPs (e.g., role-play expertise, structured communi-\\ncation, streamlined workflow) can significantly improve code generation. Other baseline methods\\n8', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='74b64a87-1915-4e1e-a00b-ce4d6889270e', embedding=None, metadata={'page_label': '9', 'file_name': 'metagpt.pdf', 'file_path': '/content/data/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-01-15', 'last_modified_date': '2025-01-15'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Preprint\\nTable 2: Comparison of capabilities for MetaGPT and other approaches. ‘!’ indicates the\\npresence of a specific feature in the corresponding framework, ‘%’ its absence.\\nFramework Capabiliy AutoGPT LangChain AgentVerse ChatDev MetaGPT\\nPRD generation % % % % !\\nTenical design genenration % % % % !\\nAPI interface generation % % % % !\\nCode generation ! ! ! ! !\\nPrecompilation execution % % % % !\\nRole-based task management % % % ! !\\nCode review % % ! ! !\\nTable 3: Ablation study on roles. ‘#’ denotes ‘The number of’, ‘Product’ denotes ‘Product man-\\nager’, and ‘Project’ denotes ‘Project manager’. ‘ !’ indicates the addition of a specific role. ‘Revi-\\nsions’ refers to ‘Human Revision Cost’.\\nEngineer Product Architect Project #Agents #Lines Expense Revisions Executability\\n! % % % 1 83.0 $ 0.915 10 1.0\\n! ! % % 2 112.0 $ 1.059 6.5 2.0\\n! ! ! % 3 143.0 $ 1.204 4.0 2.5\\n! ! % ! 3 205.0 $ 1.251 3.5 2.0\\n! ! ! ! 4 191.0 $ 1.385 2.5 4.0\\ncan easily integrate SOP-like designs to improve their performance, similar to injecting chain-of-\\nthought (Wei et al., 2022) in LLMs.\\n4.4 A BLATION STUDY\\nThe Effectiveness of Roles To understand the impact of different roles on the final results, we\\nperform two tasks that involve generating effective code and calculating average statistics. When we\\nexclude certain roles, unworkable codes are generated. As indicated by Table 3, the addition of roles\\ndifferent from just the Engineer consistently improves both revisions and executability. While more\\nroles slightly increase the expenses, the overall performance improves noticeably, demonstrating the\\neffectiveness of the various roles.\\nThe Effectiveness of Executable Feedback Mechanism As shown in Figure 4, adding executable\\nfeedback into MetaGPT leads to a significant improvement of 4.2% and 5.4% in Pass @1on Hu-\\nmanEval and MBPP, respectively. Besides, Table 1 shows that the feedback mechanism improves\\nfeasibility (3.67 to 3.75) and reduces the cost of human revisions (2.25 to 0.83). These results\\nillustrate how our designed feedback mechanism can produce higher-quality code. Additional quan-\\ntitative results of MetaGPT and MetaGPT without executable feedback are shown in Table 4 and\\nTable 9.\\n5 C ONCLUSION\\nThis work introduces MetaGPT, a novel meta-programming framework that leverages SOPs to en-\\nhance the problem-solving capabilities of multi-agent systems based on Large Language Models\\n(LLMs). MetaGPT models a group of agents as a simulated software company, analogous to simu-\\nlated towns (Park et al., 2023) and the Minecraft Sandbox in V oyager (Wang et al., 2023a). MetaGPT\\nleverages role specialization, workflow management, and efficient sharing mechanisms such as mes-\\nsage pools and subscriptions, rendering it a flexible and portable platform for autonomous agents\\nand multi-agent frameworks. It uses an executable feedback mechanism to enhance code generation\\nquality during runtime. In extensive experiments, MetaGPT achieves state-of-the-art performance\\non multiple benchmarks. The successful integration of human-like SOPs inspires future research\\non human-inspired techniques for artificial multi-agent systems. We also view our work as an early\\nattempt to regulate LLM-based multi-agent frameworks. See also the outlook (Appendix A).\\n9', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='66435a85-1b32-4cca-98e8-a7425413f69c', embedding=None, metadata={'page_label': '10', 'file_name': 'metagpt.pdf', 'file_path': '/content/data/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-01-15', 'last_modified_date': '2025-01-15'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Preprint\\nAcknowledgement\\nWe thank Sarah Salhi, the Executive Secretary of KAUST AI Initiative, and Yuhui Wang, Postdoc-\\ntoral Fellow at the KAUST AI Initiative, for helping to polish some of the text. We would like to\\nexpress our gratitude to Wenyi Wang, a PhD student at the KAUST AI Initiative, for providing com-\\nprehensive feedback on the paper and for helping to draft the outlook (Appendix A) with Mingchen.\\nWe also thank Zongze Xu, the vice president of DeepWisdom, for providing illustrative materials\\nfor AgentStore.\\nAuthor Contributions\\nSirui Hong conducted most of the experiments and designed the executable feedback module. She\\nalso led the initial version of the write-up, supported by Ceyao Zhang, and also by Jinlin Wang and\\nZili Wang. Mingchen Zhuge designed the self-improvement module, discussed additional experi-\\nments, and led the current write-up. Jonathan Chen helped with the MBPP experiments, outlined\\nthe methods section, and contributed to the current write-up. Xiawu Zheng provided valuable guid-\\nance, reviewed and edited the paper. Yuheng Cheng contributed to the evaluation metric design and\\nHumanEval experiments. Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Lingfeng Xiao helped\\nwith the MBPP experiments and comparisons to open-source baseline methods. Chenyu Ran cre-\\nated most of the illustrative figures. Chenglin Wu is the CEO of DeepWisdom, initiated MetaGPT,\\nmade the most significant code contributions to it, and advised this project. J ¨urgen Schmidhuber,\\nDirector of the AI Initiative at KAUST and Scientific Director of IDSIA, advised this project and\\nhelped with the write-up.\\nREFERENCES\\nElif Akata, Lion Schulz, Julian Coda-Forno, Seong Joon Oh, Matthias Bethge, and Eric Schulz.\\nPlaying repeated games with large language models. arXiv preprint, 2023.\\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan,\\nEllen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. Program synthesis with large\\nlanguage models, 2021.\\nAnton Bakhtin, Noam Brown, Emily Dinan, Gabriele Farina, Colin Flaherty, Daniel Fried, Andrew\\nGoff, Jonathan Gray, Hengyuan Hu, et al. Human-level play in the game of diplomacy by com-\\nbining language models with strategic reasoning. Science, 2022.\\nRobert Balzer. A 15 year perspective on automatic programming. TSE, 1985.\\nR.M. Belbin. Team Roles at Work. Routledge, 2012. URL https://books.google.co.uk/\\nbooks?id=MHIQBAAAQBAJ.\\nTianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen, and Denny Zhou. Large language models as\\ntool makers. arXiv preprint, 2023.\\nHarrison Chase. LangChain. https://github.com/hwchase17/langchain, 2022.\\nBei Chen, Fengji Zhang, Anh Nguyen, Daoguang Zan, Zeqi Lin, Jian-Guang Lou, and Weizhu\\nChen. Codet: Code generation with generated tests, 2022.\\nJiaqi Chen, Yuxian Jiang, Jiachen Lu, and Li Zhang. S-agents: self-organizing agents in open-ended\\nenvironment. arXiv preprint, 2024.\\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri,\\nGretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan,\\nScott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian,\\nClemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fo-\\ntios Chantzis, Elizabeth Barnes, Ariel Herbert-V oss, William Hebgen Guss, Alex Nichol, Alex\\nPaino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,\\nChristopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec\\nRadford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob Mc-\\nGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large\\nlanguage models trained on code, 2021a.\\n10', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='4b9435f4-e21a-4595-8f7f-ff1c0b8a1fd6', embedding=None, metadata={'page_label': '11', 'file_name': 'metagpt.pdf', 'file_path': '/content/data/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-01-15', 'last_modified_date': '2025-01-15'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Preprint\\nWeize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang, Chenfei Yuan, Chen Qian, Chi-Min Chan,\\nYujia Qin, Yaxi Lu, Ruobing Xie, Zhiyuan Liu, Maosong Sun, and Jie Zhou. Agentverse: Facili-\\ntating multi-agent collaboration and exploring emergent behaviors in agents, 2023.\\nXinyun Chen, Chang Liu, and Dawn Song. Execution-guided neural program synthesis. In ICLR,\\n2018.\\nXinyun Chen, Dawn Song, and Yuandong Tian. Latent execution for neural program synthesis\\nbeyond domain-specific languages. NeurIPS, 2021b.\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam\\nRoberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh,\\nKensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam\\nShazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James\\nBradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Lev-\\nskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin\\nRobinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret\\nZoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick,\\nAndrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica\\nMoreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Bren-\\nnan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas\\nEck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways,\\n2022.\\nT. DeMarco and T.R. Lister. Peopleware: Productive Projects and Teams. Addison-Wesley, 2013.\\nURL https://books.google.co.uk/books?id=DVlsAQAAQBAJ.\\nYihong Dong, Xue Jiang, Zhi Jin, and Ge Li. Self-collaboration code generation via chatgpt. arXiv\\npreprint, 2023.\\nYilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch. Improving\\nfactuality and reasoning in language models through multiagent debate, 2023.\\nYanai Elazar, Nora Kassner, Shauli Ravfogel, Abhilasha Ravichander, Eduard Hovy, Hinrich\\nSch¨utze, and Yoav Goldberg. Measuring and improving consistency in pretrained language mod-\\nels. TACL, 2021.\\nZhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong, Linjun Shou, Bing\\nQin, Ting Liu, Daxin Jiang, et al. Codebert: A pre-trained model for programming and natural\\nlanguages. arXiv preprint, 2020.\\nChrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Tim Rockt¨aschel.\\nPromptbreeder: Self-referential self-improvement via prompt evolution. arXiv preprint, 2023.\\nChelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation\\nof deep networks. In ICML, 2017.\\nDaniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong,\\nWen-tau Yih, Luke Zettlemoyer, and Mike Lewis. Incoder: A generative model for code infilling\\nand synthesis. arXiv preprint, 2022.\\nIrving John Good. Speculations concerning the first ultraintelligent machine. Adv. Comput., 1965.\\nRui Hao, Linmei Hu, Weijian Qi, Qingliu Wu, Yirui Zhang, and Liqiang Nie. Chatllm network:\\nMore brains, more intelligence. arXiv preprint, 2023.\\nS. Hochreiter, A. S. Younger, and P. R. Conwell. Learning to learn using gradient descent. InLecture\\nNotes on Comp. Sci. 2130, Proc. Intl. Conf. on Artificial Neural Networks (ICANN-2001), pp. 87–\\n94. Springer: Berlin, Heidelberg, 2001.\\nXue Jiang, Yihong Dong, Lecheng Wang, Qiwei Shang, and Ge Li. Self-planning code generation\\nwith large language model. arXiv preprint, 2023.\\n11', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='52585ecd-8a11-4bc5-b0e1-f256906f557d', embedding=None, metadata={'page_label': '12', 'file_name': 'metagpt.pdf', 'file_path': '/content/data/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-01-15', 'last_modified_date': '2025-01-15'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Preprint\\nGuohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and Bernard Ghanem.\\nCamel: Communicative agents for” mind” exploration of large scale language model society.\\narXiv preprint, 2023.\\nYujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R ´emi Leblond, Tom\\nEccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation\\nwith alphacode. Science, 2022.\\nTian Liang, Zhiwei He, Wenxiang Jiao, Xing Wang, Yan Wang, Rui Wang, Yujiu Yang, Zhaopeng\\nTu, and Shuming Shi. Encouraging divergent thinking in large language models through multi-\\nagent debate. arXiv preprint, 2023.\\nBill Yuchen Lin, Yicheng Fu, Karina Yang, Prithviraj Ammanabrolu, Faeze Brahman, Shiyu Huang,\\nChandra Bhagavatula, Yejin Choi, and Xiang Ren. Swiftsage: A generative agent with fast and\\nslow thinking for complex interactive tasks. arXiv preprint, 2023.\\nRuibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny Zhou, Andrew M Dai, Diyi Yang, and\\nSoroush V osoughi. Training socially aligned language models in simulated human society.arXiv\\npreprint, 2023.\\nZiyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing\\nMa, Qingwei Lin, and Daxin Jiang. Wizardcoder: Empowering code large language models with\\nevol-instruct. arXiv preprint, 2023.\\nPotsawee Manakul, Adian Liusie, and Mark JF Gales. Selfcheckgpt: Zero-resource black-box hal-\\nlucination detection for generative large language models. arXiv preprint, 2023.\\nAgile Manifesto. Manifesto for agile software development. Snowbird, UT, 2001.\\nJohn McCarthy. History of lisp. In History of programming languages. 1978.\\nAnsong Ni, Srini Iyer, Dragomir Radev, Veselin Stoyanov, Wen-tau Yih, Sida Wang, and Xi Victoria\\nLin. Lever: Learning to verify language-to-code generation with execution. In ICML, 2023.\\nErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese,\\nand Caiming Xiong. Codegen: An open large language model for code with multi-turn program\\nsynthesis, 2023.\\nOpenAI. Gpt-4 technical report, 2023.\\nJoon Sung Park, Joseph C O’Brien, Carrie J Cai, Meredith Ringel Morris, Percy Liang, and\\nMichael S Bernstein. Generative agents: Interactive simulacra of human behavior.arXiv preprint,\\n2023.\\nChen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu, and\\nMaosong Sun. Communicative agents for software development, 2023.\\nBaptiste Rozi`ere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi\\nAdi, Jingyu Liu, Tal Remez, J´er´emy Rapin, et al. Code llama: Open foundation models for code.\\narXiv preprint, 2023.\\nTimo Schick, Jane Dwivedi-Yu, Roberto Dess`ı, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer,\\nNicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to\\nuse tools. arXiv preprint, 2023.\\nJ. Schmidhuber. A self-referential weight matrix. In Proceedings of the International Conference\\non Artificial Neural Networks, Amsterdam, pp. 446–451. Springer, 1993a.\\nJ. Schmidhuber. G ¨odel machines: self-referential universal problem solvers making provably\\noptimal self-improvements. Technical Report IDSIA-19-03, arXiv:cs.LO/0309048 v3, IDSIA,\\nManno-Lugano, Switzerland, December 2003.\\nJ. Schmidhuber. G ¨odel machines: Fully self-referential optimal universal self-improvers. In B. Go-\\nertzel and C. Pennachin (eds.), Artificial General Intelligence , pp. 199–226. Springer Verlag,\\n2006. Variant available as arXiv:cs.LO/0309048.\\n12', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='9d2aca10-cf45-4550-a1b8-316b71049bf1', embedding=None, metadata={'page_label': '13', 'file_name': 'metagpt.pdf', 'file_path': '/content/data/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-01-15', 'last_modified_date': '2025-01-15'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Preprint\\nJ. Schmidhuber. Ultimate cognition `a la G¨odel. Cognitive Computation, 1(2):177–193, 2009.\\nJ¨urgen Schmidhuber. Evolutionary principles in self-referential learning, or on learning how to\\nlearn: the meta-meta-... hook. PhD thesis, 1987.\\nJ¨urgen Schmidhuber. A ‘self-referential’weight matrix. In ICANN’93: Proceedings of the Interna-\\ntional Conference on Artificial Neural Networks Amsterdam, The Netherlands 13–16 September\\n1993 3, 1993b.\\nJ¨urgen Schmidhuber. On learning to think: Algorithmic information theory for novel combinations\\nof reinforcement learning controllers and recurrent neural world models. arXiv preprint, 2015.\\nJ¨urgen Schmidhuber, Jieyu Zhao, and Nicol N Schraudolph. Reinforcement learning with self-\\nmodifying policies. In Learning to learn. 1998.\\nNoah Shinn, Beck Labash, and Ashwin Gopinath. Reflexion: an autonomous agent with dynamic\\nmemory and self-reflection. arXiv preprint, 2023.\\nMarta Skreta, Naruki Yoshikawa, Sebastian Arellano-Rubach, Zhi Ji, Lasse Bjørn Kristensen,\\nKourosh Darvish, Al ´an Aspuru-Guzik, Florian Shkurti, and Animesh Garg. Errors are useful\\nprompts: Instruction guided task programming with verifier-assisted iterative prompting. arXiv\\npreprint, 2023.\\nElliot Soloway. Learning to program = learning to construct mechanisms and explanations. Com-\\nmunications of the ACM, 1986.\\nYashar Talebirad and Amirhossein Nadiri. Multi-agent collaboration: Harnessing the power of\\nintelligent llm agents, 2023.\\nTorantulino et al. Auto-gpt. https://github.com/Significant-Gravitas/\\nAuto-GPT, 2023.\\nR. J. Waldinger and R. C. T. Lee. PROW: a step toward automatic program writing. In D. E. Walker\\nand L. M. Norton (eds.), Proceedings of the 1st International Joint Conference on Artificial Intel-\\nligence (IJCAI), 1969.\\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan,\\nand Anima Anandkumar. V oyager: An open-ended embodied agent with large language models.\\narXiv preprint, 2023a.\\nLei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai\\nTang, Xu Chen, Yankai Lin, et al. A survey on large language model based autonomous agents.\\narXiv preprint, 2023b.\\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdh-\\nery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models.\\narXiv preprint, 2022.\\nZhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. Unleashing\\ncognitive synergy in large language models: A task-solving agent through multi-persona self-\\ncollaboration. arXiv preprint, 2023c.\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. NeurIPS,\\n2022.\\nMichael Wooldridge and Nicholas R. Jennings. Pitfalls of agent-oriented development. In Pro-\\nceedings of the Second International Conference on Autonomous Agents , 1998. URL https:\\n//doi.org/10.1145/280765.280867.\\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\\nReact: Synergizing reasoning and acting in language models. arXiv preprint, 2022.\\nEric Zelikman, Eliana Lorch, Lester Mackey, and Adam Tauman Kalai. Self-taught optimizer (stop):\\nRecursively self-improving code generation. arXiv preprint, 2023.\\n13', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='be572aa0-dfc2-4555-bc34-72a11ecbd6bc', embedding=None, metadata={'page_label': '14', 'file_name': 'metagpt.pdf', 'file_path': '/content/data/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-01-15', 'last_modified_date': '2025-01-15'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Preprint\\nHongxin Zhang, Weihua Du, Jiaming Shan, Qinhong Zhou, Yilun Du, Joshua B Tenenbaum, Tian-\\nmin Shu, and Chuang Gan. Building cooperative embodied agents modularly with large language\\nmodels. arXiv preprint, 2023.\\nXufeng Zhao, Mengdi Li, Cornelius Weber, Muhammad Burhan Hafez, and Stefan Wermter. Chat\\nwith the environment: Interactive multimodal perception using large language models. arXiv\\npreprint, 2023.\\nQinkai Zheng, Xiao Xia, Xu Zou, Yuxiao Dong, Shan Wang, Yufei Xue, Zihan Wang, Lei Shen,\\nAndi Wang, Yang Li, Teng Su, Zhilin Yang, and Jie Tang. Codegeex: A pre-trained model for\\ncode generation with multilingual evaluations on humaneval-x, 2023.\\nShuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng,\\nYonatan Bisk, Daniel Fried, Uri Alon, et al. Webarena: A realistic web environment for building\\nautonomous agents. arXiv preprint, 2023.\\nMingchen Zhuge, Haozhe Liu, Francesco Faccio, Dylan R Ashley, R ´obert Csord ´as, Anand\\nGopalakrishnan, Abdullah Hamdi, Hasan Abed Al Kader Hammoud, Vincent Herrmann, Kazuki\\nIrie, et al. Mindstorms in natural language-based societies of mind. arXiv preprint, 2023.\\n14', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='739be477-d6c3-435d-bfbf-5186d46a9b99', embedding=None, metadata={'page_label': '15', 'file_name': 'metagpt.pdf', 'file_path': '/content/data/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-01-15', 'last_modified_date': '2025-01-15'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Preprint\\nA O UTLOOK\\nA.1 S ELF -IMPROVEMENT MECHANISMS\\nOne limitation of the MetaGPT version in the main text of this paper is that each software project is\\nexecuted independently. However, through active teamwork, a software development team should\\nlearn from the experience gained by developing each project, thus becoming more compatible and\\nsuccessful over time.\\nThis is somewhat related to the idea of recursive self-improvement, first informally proposed in\\n1965 (Good, 1965), with first concrete implementations since 1987 (Schmidhuber, 1987; 1993b;\\nSchmidhuber et al., 1998), culminating in the concept of mathematically optimal self-referential\\nself-improvers (Schmidhuber, 2003; 2009). Generally speaking, a system should learn from experi-\\nence in the real world, and meta-learn better learning algorithms from experiences of learning, and\\nmeta-meta-learn better meta-learning algorithms from experiences of meta-learning, etc., without\\nany limitations except those of computability and physics.\\nMore recent, somewhat related work leverages the reasoning ability of Large Language Models\\n(LLMs) and recursively improves prompts of LLMs, to improve performance on certain downstream\\ntasks (Fernando et al., 2023; Zelikman et al., 2023), analogous to the adaptive prompt engineer of\\n2015 (Schmidhuber, 2015) where one neural network learns to generate sequence of queries or\\nprompts for another pre-trained neural network whose answers may help the first network to learn\\nnew tasks more quickly.\\nIn our present work, we also explore a self-referential mechanism that recursively modifies the con-\\nstraint prompts of agents based on information they observe during software development. Our\\ninitial implementation works as follows. Prior to each project, every agent in the software company\\nreviews previous feedback and makes necessary adjustments to their constraint prompts. This en-\\nables them to continuously learn from past project experiences and enhance the overall multi-agent\\nsystem by improving each individual in the company. We first establish ahandover feedback action\\nfor each agent. This action is responsible for critically summarizing the information received dur-\\ning the development of previous projects and integrating this information in an updated constraint\\nprompt. The summarized information is stored in long-term memory such that it can be inherited\\nby future constraint prompt updates. When initiating a new project, each agent starts with a react\\naction. Each agent evaluates the received feedback and summarizes how they can improve in a\\nconstraint prompt.\\nOne current limitation is that these summary-based optimizations only modify constraints in the\\nspecialization of roles (Sec. 3.1) rather than structured communication interfaces in communication\\nprotocols (Sec. 3.2). Future advancements are yet to be explored.\\nA.2 M ULTI -AGENT ECONOMIES\\nIn real-world teamwork, the interaction processes are often not hardcoded. For example, in a soft-\\nware company, the collaboration SOP may change dynamically.\\nOne implementation of such self-organization is discussed in the paper on a “Natural Language-\\nBased Society of Mind” (NLSOM) (Zhuge et al., 2023), which introduced the idea of an “Economy\\nof Minds” (EOM), a Reinforcement Learning (RL) framework for societies of LLMs and other\\nagents. Instead of using standard RL techniques to optimize the total reward of the system through\\nmodifications of neural network parameters, EOMs use the principles of supply and demand in free\\nmarkets to assign credit (money) to those agents that contribute to economic success (reward).\\nThe recent agent-based platform of DeepWisdom (AgentStore 4) is compatible with the credit as-\\nsignment concept of EOMs. Each agent in AgentStore provides a list of services with corresponding\\ncosts. A convenient API is provided so that human users or agents in the platform can easily pur-\\nchase services from other agents to accomplish their services. Figure 6 displays the User Interface\\n(UI) of AgentStore, where various agents with different skills are showcased. Besides, individual\\ndevelopers can participate in building new agents and enable collaborative development within the\\ncommunity. Specifically, AgentStore allows users to subscribe to agents according to their demands\\n4http://beta.deepwisdom.ai\\n15', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='e7990cb0-c522-46e2-8da7-2e2108c57109', embedding=None, metadata={'page_label': '16', 'file_name': 'metagpt.pdf', 'file_path': '/content/data/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-01-15', 'last_modified_date': '2025-01-15'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Preprint\\nand pay according to their usage. Moreover, users can purchase additional capabilities to expand the\\nplug-and-play functions of their existing agents. This allows users to gradually upgrade their agents.\\nWithin the MetaGPT framework, AgentStore can support the collaboration of various agents. Users\\ncan collect several agents together to carry out more complex tasks or projects, and all the agents\\nshare and comply with development and communication protocols defined in MetaGPT.\\nFigure 6: AgentStore is a platform dedicated to serving users in the creation and development of\\nagents within the MetaGPT framework. This platform provides users with an operational interface,\\nallowing users to easily manage a variety of agents with different emotions, personalities, and capa-\\nbilities for specific tasks.\\n16', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='54f90186-0efd-41b8-8cb7-079059cffe2a', embedding=None, metadata={'page_label': '17', 'file_name': 'metagpt.pdf', 'file_path': '/content/data/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-01-15', 'last_modified_date': '2025-01-15'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Preprint\\nB A D EMO OF THE EXECUTION\\nIn this section, we outline the complete process of software development using MetaGPT. It begins\\nwith a user’s input command (as shown in Appendix B.1) and ends with software designed according\\nto the user’s specifications.\\nB.1 U SER INPUT\\nUpon receiving an instruction from the user, MetaGPT collaborates with a professional development\\nteam to fulfill the task. Here is a demo of user input:\\nUser input\\n1 write a python3 GUI app such that you can draw an image with it\\nB.2 M ETAGPT DEVELOPMENT PROCESS\\nNow we provide a step-by-step explanation of the standardized output process for each agent.\\nProduct Manager The Product Manager generates a Product Requirement Document (PRD), as\\ndetailed in the specified documentation. This document encompasses goals, user stories, competitive\\nanalysis, requirement analysis and requirement pool. Additionally, a competitive quadrant chart is\\nproduced (see Figure 7). Subsequently, these documents and charts are handed over to the architect\\nfor system design.\\nProduct Requirement Document\\n1 ## Original Requirements\\n2 The boss requires a Python3 GUI color meter that outputs the RGB values when it moves.\\n3\\n4 ## Product Goals\\n5 ‘‘‘python\\n6 [\\n7 \"Create a user-friendly GUI color meter\",\\n8 \"Ensure the color meter accurately outputs RGB values\",\\n9 \"Ensure the color meter updates RGB values in real-time as it moves\"\\n10 ]\\n11 ‘‘‘\\n12\\n13 ## User Stories\\n14 ‘‘‘python\\n15 [\\n16 \"As a user, I want to select any color on the screen, so that I can get its RGB values\",\\n17 \"As a user, I want the RGB values to update in real-time as I move the color meter, so that I\\n,→ can get accurate color information\",\\n18 \"As a user, I want a simple and intuitive interface, so that I can easily use the color meter\"\\n19 ]\\n20 ‘‘‘\\n21\\n22 ## Competitive Analysis\\n23 ‘‘‘python\\n24 [\\n25 \"Color Cop: A popular color picker for Windows. It’s simple and easy to use, but lacks real-\\n,→ time RGB value updates\",\\n26 \"Just Color Picker: Offers real-time color picking and multiple color code formats. However,\\n,→ its interface is cluttered and not very user-friendly\",\\n27 \"Instant Eyedropper: A simple color picker that lives in the system tray. It’s convenient but\\n,→ lacks advanced features\",\\n28 \"ColorZilla: A color picker extension for browsers. It’s convenient for web design but not\\n,→ suitable for desktop applications\",\\n29 \"ColorPic: Offers color palettes and mixer tools. It’s feature-rich but can be overwhelming\\n,→ for simple tasks\"\\n30 ]\\n31 ‘‘‘\\n17', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='3492d977-9bf0-4e65-8364-98aef549e056', embedding=None, metadata={'page_label': '18', 'file_name': 'metagpt.pdf', 'file_path': '/content/data/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-01-15', 'last_modified_date': '2025-01-15'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Preprint\\nProduct Requirement Document\\n1\\n2 ## Requirement Analysis\\n3 The product should be a Python3 GUI application that serves as a color meter. It should allow\\n,→ users to select any color on the screen and display the corresponding RGB values. The RGB\\n,→ values should update in real-time as the color meter moves. The interface should be\\n,→ simple and user-friendly.\\n4\\n5 ## Requirement Pool\\n6 ‘‘‘python\\n7 [\\n8 (\"Design a user-friendly GUI for the color meter\", \"P0\"),\\n9 (\"Implement color selection functionality\", \"P0\"),\\n10 (\"Display RGB values of the selected color\", \"P0\"),\\n11 (\"Update RGB values in real-time as the color meter moves\", \"P0\"),\\n12 (\"Test the application for accuracy and performance\", \"P1\")\\n13 ]\\n14 ‘‘‘\\n15\\n16 ## UI Design draft\\n17 The UI should be minimalistic and intuitive. It should primarily consist of a color meter, which\\n,→ could be a magnified area of the screen that the user can move around. The RGB values of\\n,→ the currently selected color should be prominently displayed, updating in real-time as\\n,→ the color meter moves. The layout should be clean and uncluttered, with a focus on\\n,→ functionality and ease of use.\\n18\\n19 ## Anything UNCLEAR\\n20 There are no unclear points.\\nFigure 7: The quadrant chart for the “Drawing App” generated by the product manager agent.\\n18', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='2fb5a256-b15b-4a72-8219-eec2e2812484', embedding=None, metadata={'page_label': '19', 'file_name': 'metagpt.pdf', 'file_path': '/content/data/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-01-15', 'last_modified_date': '2025-01-15'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Preprint\\nArchitect Based on the requirements in PRD, the Architect agent devises technical specifications\\nincluding system architecture diagrams and interface definitions. Initially, the Architect defines the\\noverarching technical trajectory. Subsequently, the project’s architecture, including files, classes\\n(Figure 8) and the sequence flow chart (Figure 12), is designed. The Architect’s documentation is\\nthen given to the project manager for task allocation and execution.\\nSystem design\\n1 ## Implementation approach\\n2 We will use Python’s Tkinter library to create the GUI for the color meter. Tkinter is a standard\\n,→ Python interface to the Tk GUI toolkit and is included with most Python installations. It\\n,→ is simple to use and powerful enough for our needs.\\n3\\n4 The color selection functionality will be implemented using the PIL (Pillow) library, which allows\\n,→ us to capture the screen’s pixel data. By moving the mouse, we can get the RGB values of\\n,→ the pixel under the mouse cursor in real-time.\\n5\\n6 The RGB values will be displayed on the GUI using Tkinter’s Label widget. We will use Python’s\\n,→ threading library to update the RGB values in real-time as the color meter moves.\\n7\\n8 The application will be tested for accuracy and performance using Python’s unittest library.\\n9\\n10 ## Python package name\\n11 ‘‘‘python\\n12 \"color_meter\"\\n13 ‘‘‘\\n14\\n15 ## File list\\n16 ‘‘‘python\\n17 [\\n18 \"main.py\",\\n19 \"color_picker.py\",\\n20 \"gui.py\",\\n21 \"tests.py\"\\n22 ]\\n23 ‘‘‘\\n24\\n25 ## Anything UNCLEAR\\n26 The requirement is clear to me.\\nFigure 8: Data structures and interface definitions for the “Drawing App” generated by thearchitect\\nagent.\\nProject Manager The Project Manager breaks down the project into a task list. Furthermore, each\\ncode file is analyzed based on its intended functionality and then treated as a separate task assigned\\nto Engineers.\\n19', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='c5f7799b-f740-4346-a1d8-a8991189f769', embedding=None, metadata={'page_label': '20', 'file_name': 'metagpt.pdf', 'file_path': '/content/data/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-01-15', 'last_modified_date': '2025-01-15'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Preprint\\nFigure 9: The program call flow for the “Drawing App” generated by the architect agent.\\nTasks\\n1 ## Required Python third-party packages\\n2\\n3 ‘‘‘python\\n4 \"\"\"\\n5 tkinter==8.6\\n6 Pillow==8.3.1\\n7 \"\"\"\\n8 ‘‘‘\\n9\\n10 ## Required Other language third-party packages\\n11 ‘‘‘python\\n12 \"\"\"\\n13 No third-party packages required in other languages.\\n14 \"\"\"\\n15 ‘‘‘\\n16\\n17 ## Full API spec\\n18 ‘‘‘python\\n19 \"\"\"\\n20 No APIs are used in this project.\\n21 \"\"\"\\n22 ‘‘‘\\n23\\n24 ## Logic Analysis\\n25 ‘‘‘python\\n26 [\\n27 (\"main.py\", \"Contains the main application loop and initializes all the other components.\"),\\n28 (\"canvas.py\", \"Implements the drawing canvas, responds to mouse events, and uses the selected\\n,→ tool and color.\"),\\n29 (\"tools.py\", \"Implements the tool selection interface.\"),\\n30 (\"color_picker.py\", \"Implements the color selection interface.\"),\\n31 (\"file_manager.py\", \"Implements the file saving and opening functionality.\")\\n32 ]\\n33 ‘‘‘\\n20', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='dced2dec-2dc1-45c3-8f4f-50803d89b1d7', embedding=None, metadata={'page_label': '21', 'file_name': 'metagpt.pdf', 'file_path': '/content/data/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-01-15', 'last_modified_date': '2025-01-15'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Preprint\\nProduct Requirement Document\\n1 ## Task list\\n2 ‘‘‘python\\n3 [\\n4 \"main.py\",\\n5 \"canvas.py\",\\n6 \"tools.py\",\\n7 \"color_picker.py\",\\n8 \"file_manager.py\"\\n9 ]\\n10 ‘‘‘\\n11\\n12 ## Shared Knowledge\\n13 ‘‘‘python\\n14 \"\"\"\\n15 The ’Tkinter’ library is used for creating the GUI of the application. It provides various widgets\\n,→ like frames and canvases which are used in this project.\\n16 The ’Pillow’ library is used for handling image files. It is used in the ’file_manager.py’ for\\n,→ saving and opening image files.\\n17 The ’main.py’ file is the entry point of the application. It initializes all the other components\\n,→ and starts the application loop.\\n18 \"\"\"\\n19 ‘‘‘\\n20\\n21 ## Anything UNCLEAR\\n22 The project requirements and implementation approach are clear. However, we need to ensure that\\n,→ all team members are familiar with the Tkinter and Pillow libraries. If not, they may\\n,→ need some time to learn these libraries before starting the development.\\n21', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='c6ca73be-f276-411a-af14-1f383ba24e67', embedding=None, metadata={'page_label': '22', 'file_name': 'metagpt.pdf', 'file_path': '/content/data/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-01-15', 'last_modified_date': '2025-01-15'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Preprint\\nEngineer Given the provided file structure and function definitions, an Engineer agent requires\\nonly fundamental development skills to complete the development tasks. Due to the large number\\nof files, we present only one auto-generated code file here.\\nGenerated code (color picker.py)\\n1 import tkinter as tk\\n2 from tkinter import colorchooser\\n3 class ColorPicker:\\n4 def __init__(self, root: tk.Tk, color: str = ’black’):\\n5 self.frame = tk.Frame(root)\\n6 self.color = color\\n7\\n8 self.color_button = tk.Button(self.frame, text=’Color’, command=self.select_color)\\n9 self.color_button.pack(fill=’x’)\\n10\\n11 def select_color(self):\\n12 color = colorchooser.askcolor()[1]\\n13 if color is not None:\\n14 self.color = color\\n15\\n16 def pack(self, **kwargs):\\n17 self.frame.pack(**kwargs)\\n18\\n19 def get_color(self):\\n20 return self.color\\nQA Engineer Upon receiving the code output from the Engineer, the QA Engineer generates unit\\ntest code and reviews it to identify and fix any bugs, ensuring high-quality software.\\nGenerated Unit tests (test.py)\\n1 import unittest\\n2 import tkinter as tk\\n3 # You might want to move the ColorPicker class to another file, let’s say color_picker.py\\n4 # from color_picker import ColorPicker\\n5\\n6 class TestColorPicker(unittest.TestCase):\\n7\\n8 def setUp(self):\\n9 self.root = tk.Tk()\\n10 self.cp = ColorPicker(self.root)\\n11\\n12 def tearDown(self):\\n13 # This is important to close the tk window after tests\\n14 self.root.destroy()\\n15\\n16 def test_initial_color(self):\\n17 # Test the default color during initialization\\n18 self.assertEqual(self.cp.get_color(), ’black’)\\n19\\n20 def test_set_and_get_color(self):\\n21 # Test setting a new color and retrieving it\\n22 new_color = ’#ffffff’ # white color\\n23 self.cp.color = new_color\\n24 self.assertEqual(self.cp.get_color(), new_color)\\n25\\n26\\n27 if __name__ == ’__main__’:\\n28 unittest.main()\\nOutput Ultimately, as shown in Figure 10, MetaGPT generates a functional application named\\n“Drawing App”.\\n22', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='d5a2025c-3b49-4f4f-b80a-a78c2543bc23', embedding=None, metadata={'page_label': '23', 'file_name': 'metagpt.pdf', 'file_path': '/content/data/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-01-15', 'last_modified_date': '2025-01-15'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Preprint\\nFigure 10: The “Drawing App” generated by MetaGPT.\\nC E XPERIMENTS\\nC.1 D ETAILS OF THE SOFTWARE DEV DATASET\\nThe SoftwareDev dataset includes 70 diverse software development tasks. Table 8 displays the\\nnames and detailed prompts of 11 tasks within the dataset. Note that the first seven tasks listed are\\nused in the main experiments of this paper.\\nC.2 A DDITIONAL RESULTS\\nQuantitative results of MetaGPT As shown in Table 4, MetaGPT achieves an average score\\nof 3.9, surpassing ChatDev’s score of 2.1 Zhao et al. (2023), which is based on the Chat chain.\\nCompare the scores of general intelligent algorithms, including AutoGPT Torantulino et al. (2023),\\nwhich all score 1.0, failing to generate executable code. We observe that the generated code is often\\nshort, lacks comprehensive logic, and tends to fail to handle cross-file dependencies correctly.\\nWhile models such as AutoGPT (Torantulino et al., 2023), Langchain (Chase, 2022), and Agent-\\nVerse (Chen et al., 2023) display robust general problem-solving capabilities, they lack an essential\\nelement for developing complex systems: systematically deconstructing requirements. Conversely,\\nMetaGPT simplifies the process of transforming abstract requirements into detailed class and func-\\ntion designs through a specialized division of labor and SOPs workflow. When compared to Chat-\\nDev (Zhao et al., 2023), MetaGPT’s structured messaging and feedback mechanisms not only reduce\\nloss of communication information but also improve the execution of code.\\nQuantitative results of MetaGPT w/o executable feedback Table 9 presents the performance of\\nMetaGPT with GPT-4 32K on 11 tasks within the SoftwareDev dataset. It also shows the average\\nperformance across all 70 tasks (in the last line). Note that the version of MetaGPT used here is the\\nbasic version without the executable feedback mechanism.\\nQuantitative results of MetaGPT with different LLMs To verify the performance of MetaGPT\\non different LLMs, we randomly selected 5 SoftwareDev tasks and conducted experiments using\\nGPT-3.5 and Deepseek Coder 33B 5 as backends. As shown in Table 5, the results indicate that\\nalthough MetaGPT can complete tasks with these LLMs, using GPT-4 as the backend yields superior\\nperformance.\\n5https://deepseekcoder.github.io\\n23', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='915135b5-1f4a-40db-89ed-a83715772299', embedding=None, metadata={'page_label': '24', 'file_name': 'metagpt.pdf', 'file_path': '/content/data/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-01-15', 'last_modified_date': '2025-01-15'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Preprint\\nTable 4: Executability comparison. The executability scores are on a grading system ranging from\\n’1’ to ’4’. A score of ’1’ signifies complete failure, ’2’ denotes executable code, ’3’ represents\\nlargely satisfying expected workflow, and ’4’ indicates a perfect match with expectations.\\nTask AutoGPT LangChain AgentVerse ChatDev MetaGPT\\nFlappy bird 1 1 1 2 3\\nTank battle game 1 1 1 2 4\\n2048 game 1 1 1 1 4\\nSnake game 1 1 1 3 4\\nBrick breaker game 1 1 1 1 4\\nExcel data process 1 1 1 4 4\\nCRUD manage 1 1 1 2 4\\nAverage score 1.0 1.0 1.0 2.1 3.9\\nTable 5: Performance of MetaGPT on SoftwareDev using different LLMs as agent backends.\\nModel Open source Time(/s) # Lines Executability Revisions\\nMetaGPT (w/ GPT-3.5) % 75.18 161.6 2.8 2.4\\nMetaGPT (w/ GPT-4) % 552.94 178.2 3.8 1.2\\nMetaGPT (w/ Deepseek Coder 33B) \" 1186.20 120.2 1.4 2.6\\nImpact of Instruction Levels (High-level v.s. Detailed Instructions) Does the variation in the\\nlevel of initial input from humans significantly influence performance outcomes? For examples:\\n1. High-level prompt: Create a brick breaker game.\\n2. Detailed prompt: Creating a brick breaker game. In a brick breaker game, the player\\ntypically controls a paddle at the bottom of the screen to bounce a ball towards a wall of\\nbricks. The goal is to break all the bricks by hitting them with the ball.\\nAdditional experiments were conducted to investigate this aspect: we selected 5 tasks from Soft-\\nwareDev, and constructed detailed prompts for them. Here are the experimental results:\\nTable 6: Impact of Instruction Levels. The executability is scored on a grading system ranging\\nfrom ‘1’ to ‘4’. A score of ‘1’ signifies complete failure, ‘2’ denotes runnable code, ‘3’ represents\\nlargely expected workflow, and ‘4’ indicates a perfect match to expectations.\\nModel # Word Time(/s) Token usage # Lines Executability Productivity Reversions\\nHigh-level 13.2 552.9 28384.2 178.2 3.8 163.8 1.2\\nDetailed 42.2 567.8 29657.0 257.0 4.0 118.0 1.6\\nWe observe that: detailed prompts lead to better software projects with lower productivity ratios\\nbecause of clearer requirements and functions, while simple inputs can still generate good enough\\nsoftware using MetaGPT with an executability rating of 3.8, which is comparable to the detailed\\nprompt scenario. (Note that, Productivity = Token usage / Total Code Lines. The lower this ratio,\\nthe better.)\\nThe performance of GPT variants in HumanEval benchmark We use the GPT-4’s 67% Hu-\\nmanEval score (OpenAI, 2023) as our baseline, acknowledging its acceptance in the HumanEval\\nbenchmark. We further extend to experiments(five times) with GPT-4 (gpt-4-0613) and GPT-3.5-\\nTurbo (gpt-3.5-turbo-0613) under various conditions to assess performance. (A) We directly called\\nthe OpenAI API with the prompt in HumanEval. (B) We called the OpenAI API and parsed the\\ncode with regex in the response. (C) We added an additional system prompt, then called the OpenAI\\nAPI. The prompt is ”You are an AI that only responds with Python code, NOT ENGLISH. You will\\n24', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='ca9c9a5e-b27b-4457-b4cf-13e3a574a9c3', embedding=None, metadata={'page_label': '25', 'file_name': 'metagpt.pdf', 'file_path': '/content/data/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-01-15', 'last_modified_date': '2025-01-15'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Preprint\\nbe given a function signature and its docstring by the user. Write your full implementation (restate\\nthe function signature).” As shown in Table 7, GPT-4 is more sensitive to prompt, code parser, and\\npost-processing results on the HumanEval data set. It is difficult for GPT-3.5-Turbo to return the\\ncorrect completion code without prompt words.\\nTable 7: Performance of GPT models on HumanEval. Experiments were conducted five times\\nusing gpt-4-0613 and gpt-3.5-turbo-0613 with different settings.\\nSettings Model 1 2 3 4 5 Avg. Std.\\nA gpt-4-0613 0.732 0.707 0.732 0.713 0.738 0.724 0.013\\nA gpt-3.5-turbo-0613 0.360 0.366 0.360 0.348 0.354 0.357 0.007\\nB gpt-4-0613 0.787 0.811 0.817 0.829 0.817 0.812 0.016\\nB gpt-3.5-turbo-0613 0.348 0.354 0.348 0.335 0.348 0.346 0.007\\nC gpt-4-0613 0.805 0.805 0.817 0.793 0.780 0.800 0.014\\nC gpt-3.5-turbo-0613 0.585 0.567 0.573 0.579 0.579 0.577 0.007\\nQualitative results Figure 11 and Figure 12 illustrate the outcomes of the Architect agent’s ef-\\nforts to design a complex recommender system. These figures showcase the comprehensive system\\ninterface design and program call flow. The latter is essential for creating a sophisticated automated\\nsystem. It is crucial to emphasize the importance of this division of labor in developing an automated\\nsoftware framework.\\nD L IMITATION AND ETHICS CONCERNS\\nD.1 L IMITATION\\nSystem side At present, our system cannot fully cater to specific scenarios, such as UI and front-\\nend, as we have yet to incorporate such agents and multimodal tools. Furthermore, despite gen-\\nerating the most amount of code among comparable frameworks, it remains challenging to fulfill\\nreal-world applications’ diverse and complex requirements.\\nHuman user side A key challenge for users is to interrupt the running process of each agent, or\\nset the starting running point (checkpoint) for each agent.\\nD.2 E THICS CONCERNS\\nUnemployment and Skill Obsolescence MetaGPT enables more people to program in natural\\nlanguages, thereby making it easier for engineers to get started. Over the years, programming\\nlanguages have evolved from punched cards to assembly, C, Java, Python, and now natural lan-\\nguage. As a result, humans have become more proficient at programming, increasing the demand\\nfor programming-related positions. Furthermore, programming with natural language may offer a\\nsignificantly easier learning curve, making programming more accessible to a broader audience.\\nTransparency and Accountability MetaGPT is an open-source framework that facilitates inter-\\nactive communication between multiple agents through natural language. Humans can initiate, ob-\\nserve, and stop running with the highest level of control. It provides real-time interpretation and op-\\neration of the natural language, displayed on the screen and logs, ensuring transparency. MetaGPT\\nenhances “natural language programming” capabilities, and human engineers are the users and re-\\nsponsible for the outcomes.\\nPrivacy and Data Security MetaGPT operates locally, ensuring user data privacy and security. It\\ndoes not collect user data. For interactions with third-party LLMs, such as those by OpenAI, users\\nare encouraged to refer to the respective privacy policies (e.g., OpenAI Privacy Policy). However,\\nwe provide the option of open-source LLMs as backends.\\n25', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='8993d951-0f70-4261-abdc-682048984d61', embedding=None, metadata={'page_label': '26', 'file_name': 'metagpt.pdf', 'file_path': '/content/data/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-01-15', 'last_modified_date': '2025-01-15'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Preprint\\nFigure 11: The system interface design for “recommendation engine development” is generated by\\nthe architect agent (zoom in for a better view).\\nE M ORE DISCUSSIONS\\nE.1 D EEP -SEATED CHALLENGES\\nMetaGPT also alleviates or solves these challenges with its unique designs:\\nUse Context Efficiently Two sub-challenges are present. First, unfolding short natural language\\ndescriptions accurately to eliminate ambiguity. Second, maintaining information validity in lengthy\\ncontexts, enables LLMs to concentrate on relevant data without distraction.\\nReduce Hallucinations Using LLMs to generate entire software programs faces code halluci-\\nnation problems—-including incomplete implementation of functions, missing dependencies, and\\npotential undiscovered bugs, which may be more serious. LLMs often struggle with software gen-\\neration due to vague task definitions. Focusing on granular tasks like requirement analysis and\\npackage selection offers guided thinking, which LLMs lack in broad task solving.\\nE.2 I NFORMATION OVERLOAD\\nIn MetaGPT, we use a global message pool and a subscription mechanism to address “information\\noverload,” which refers to the problem of receiving excessive or irrelevant information. This issue\\nis dependent on specific applications. MetaGPT employs a message pool to streamline communi-\\ncation, ensuring efficiency. Additionally, a subscription mechanism filters out irrelevant contexts,\\nenhancing the relevance and utility of the information. This design is particularly crucial in soft-\\n26', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='b96e5673-d1ee-4c99-a608-73688dc68dd1', embedding=None, metadata={'page_label': '27', 'file_name': 'metagpt.pdf', 'file_path': '/content/data/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-01-15', 'last_modified_date': '2025-01-15'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Preprint\\nFigure 12: The program call flow for “recommendation engine development” generated by the\\narchitect agent (zoom in for a better view).\\nware design scenarios and standard operating procedures (SOPs) where effective communication is\\nessential.\\n27', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='3b6d148c-a240-4572-9f50-fd11acd4bdf4', embedding=None, metadata={'page_label': '28', 'file_name': 'metagpt.pdf', 'file_path': '/content/data/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-01-15', 'last_modified_date': '2025-01-15'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Preprint\\nTable 8: Examples of SoftwareDev dataset.\\nTask ID Task Prompt0 Snake game Create a snake game.1 Brick breaker game Create a brick breaker game.2 2048 game Create a 2048 game for the web.3 Flappy bird game Write p5.js code for Flappy Bird where you control a yellow bird continu-ously flying between a series of green pipes. The bird flaps every time youleft click the mouse. If it falls to the ground or hits a pipe, you lose. Thisgame goes on indefinitely until you lose; you get points the further you go.4 Tank battle game Create a tank battle game.5 Excel data process Write an excel data processing program based on streamlit and pandas. Thescreen first shows an excel file upload button. After the excel file is uploaded,use pandas to display its data content. The program is required to be concise,easy to maintain, and not over-designed. It uses streamlit to process webscreen displays, and pandas is sufficient to process excel reading and display.Please make sure others can execute directly without introducing additionalpackages.6 CRUD manage Write a management program based on the crud addition, deletion, modifi-cation and query processing of the customer business entity. The customerneeds to save this information: name, birthday, age, sex, and phone. The datais stored in client.db, and there is a judgement whether the customer table ex-ists. If it doesn’t, it needs to be created first. Querying is done by name; samefor deleting. The program is required to be concise, easy to maintain, and notover-designed. The screen is realized through streamlit and sqlite—no needto introduce other additional packages.7 Music transcriber Develop a program to transcribe sheet music into a digital format; provid-ing error-free transcribed symbolized sheet music intelligence from audiothrough signal processing involving pitch and time slicing then training aneural net to run Onset Detected CWT transforming scalograms to chroma-grams decoded with Recursive Neural Network focused networks.8 Custom press releases Create custom press releases; develop a Python script that extracts rele-vant information about company news from external sources, such as socialmedia; extract update interval database for recent changes. The programshould create press releases with customizable options and export writingsto PDFs, NYTimes API JSONs, media format styled with interlink internalfixed character-length metadata.9 Gomoku game Implement a Gomoku game using Python, incorporating an AI opponentwith varying difficulty levels.10 Weather dashboard Create a Python program to develop an interactive weather dashboard.\\n28', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'), Document(id_='bc9de21d-bb53-48b8-ae7a-06733dabcd1d', embedding=None, metadata={'page_label': '29', 'file_name': 'metagpt.pdf', 'file_path': '/content/data/metagpt.pdf', 'file_type': 'application/pdf', 'file_size': 16911937, 'creation_date': '2025-01-15', 'last_modified_date': '2025-01-15'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Preprint\\nTable 9: Additional results of pure MetaGPT w/o feedback on SoftwareDev. Averages (Avg.) of 70 tasks are calculated and 10 randomly selected tasks are\\nincluded. ‘#’ denotes ‘The number of’, while ‘ID’ is ‘Task ID’.\\nID Code statistics Doc statistics Cost statistics Cost of revision Code executability\\n#code files #lines of code #lines per code file #doc files #lines of doc #lines per doc file #prompt tokens #completion tokens time costs money costs\\n0 5.00 196.00 39.20 3.00 210.00 70.00 24087.00 6157.00 582.04 $ 1.09 1. TypeError 4\\n1 6.00 191.00 31.83 3.00 230.00 76.67 32517.00 6238.00 566.30 $ 1.35 1. TypeError 4\\n2 3.00 198.00 66.00 3.00 235.00 78.33 21934.00 6316.00 553.11 $ 1.04 1. lack\\n@app.route(’/’)\\n3\\n3 5.00 164 32.80 3.00 202.00 67.33 22951.00 5312.00 481.34 $ 1.01 1. PNG file miss-\\ning 2. Compile bug\\nfixes\\n2\\n4 6.00 203.00 33.83 3.00 210.00 70.00 30087.00 6567.00 599.58 $ 1.30 1. PNG file\\nmissing 2. Com-\\npile bug fixes 3.\\npygame.surface not\\ninitialize\\n3\\n5 6.00 219.00 36.50 3.00 294.00 96.00 35590.00 7336.00 585.10 $ 1.51 1. dependency er-\\nror 2. ModuleNot-\\nFoundError\\n4\\n6 4.00 73.00 18.25 3.00 261.00 87.00 25673.00 5832.00 398.83 $ 0.90 0 4\\n7 4.00 316.00 79.00 3.00 332.00 110.67 29139.00 7104.00 435.83 $ 0.92 0 4\\n8 5.00 215.00 43.00 3.00 301.00 100.33 29372.00 6499.00 621.73 $ 1.27 1. tensorflow ver-\\nsion error 2. model\\ntraining method not\\nimplement\\n2\\n9 5.00 215.00 43.00 3.00 270.00 90.00 24799.00 5734.00 550.88 $ 1.27 1. dependency er-\\nror 2. URL 403 er-\\nror\\n3\\n10 3.00 93.00 31.00 3.00 254.00 84.67 24109.00 5363.00 438.50 $ 0.92 1. dependency er-\\nror 2. missing main\\nfunc.\\n4\\nAvg. 4.71 191.57 42.98 3.00 240.00 80.00 26626.86 6218.00 516.71 $1.12 0.51 (only consider\\nitem scored 2, 3 or\\n4)\\n3.36\\n29', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 向量化"
      ],
      "metadata": {
        "id": "upKpJWdH2I0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 若資料夾存在則刪除\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "if os.path.exists(\"./vector\"):\n",
        "    shutil.rmtree(\"./vector\")"
      ],
      "metadata": {
        "id": "gVoX72C6PyNI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "import openai\n",
        "from llama_index.core import VectorStoreIndex\n",
        "from llama_index.core import Settings, PromptTemplate\n",
        "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
        "from llama_index.core import StorageContext\n",
        "import chromadb\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "openai.api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "# 使用OpenAI的嵌入模型將文本轉換為嵌入向量\n",
        "embed_model = OpenAIEmbedding(embed_batch_size=10)\n",
        "\n",
        "system_prompt = (\n",
        "    \"你是一個專業的助理，請從給定內容中提取準確答案。\"\n",
        "    \"{context}\"\n",
        ")\n",
        "prompt_template = PromptTemplate(system_prompt)\n",
        "\n",
        "# 設定Chroma向量資料庫\n",
        "db = chromadb.PersistentClient(path=\"./vector\")\n",
        "chroma_collection = db.get_or_create_collection(\"llamaindex_chroma\")\n",
        "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
        "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
        "\n",
        "# 建立向量索引，從文本文件(documents)生成的向量\n",
        "print(\"正在構建向量索引...\")\n",
        "index = VectorStoreIndex.from_documents(\n",
        "    documents,\n",
        "    storage_context=storage_context,\n",
        "    embed_model=embed_model,\n",
        "    show_progress=True,\n",
        ")\n",
        "print(\"向量索引構建完成。\")\n",
        "\n",
        "# 將索引轉換為查詢引擎\n",
        "query_engine = index.as_query_engine(prompt_template=prompt_template)\n",
        "print(\"查詢引擎建立完成:\", query_engine)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153,
          "referenced_widgets": [
            "86e0b23b919e4216bc162e4e22f8245e",
            "9c4bf79574344f2bacfbc1413040c044",
            "ec845b1d5a594464b7c91e5baea07385",
            "56b1f4b14509453da920a2e9237d9a00",
            "6230d9061c2b43e680d499aee1a8951e",
            "da11c47825174a25b073ec430cdbd997",
            "c91210696afd4e808ae80f41c0f76d0c",
            "bed4a1e8a4d54f1d8e9ee75953a7dfe1",
            "d785312fa66e4c839d08d880b747f06b",
            "4fdaf36804b94a889bbc6d3cbba230ad",
            "768a7c6f2882411daa3d8904fa9760c1",
            "b85fd49951604b28a3ed56a62d041d44",
            "d983748325ba445db18411c5ca96fc7e",
            "bdf60a26b1bf4a5ab081ad123ac2f3cc",
            "79ae718714184cd18cec92b2ac9142bf",
            "88c43e5e175142eb9b4fea5553991e11",
            "e0056d36ca0b4401a9c7661440789a42",
            "a34e480186c045e6a628ff865b713f33",
            "2652efc27a154ef39d8399545f841213",
            "c6d6979594bd4cb0ae469cd7473ce8f5",
            "d731acef8f5f4a2e9ea8a050f1a251c1",
            "4257fb69eb584294b74d28e56fe5cc7d"
          ]
        },
        "id": "TLo5rf4_2J0j",
        "outputId": "5e5fe582-56c3-4322-e082-9e0610614d83"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "正在構建向量索引...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Parsing nodes:   0%|          | 0/29 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "86e0b23b919e4216bc162e4e22f8245e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating embeddings:   0%|          | 0/34 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b85fd49951604b28a3ed56a62d041d44"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "向量索引構建完成。\n",
            "查詢引擎建立完成: <llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine object at 0x7eebf67d45b0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 向量資料庫"
      ],
      "metadata": {
        "id": "-Y-zfozMdqvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 從存儲上下文中獲取檢索器\n",
        "retriever = index.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "# 執行相似性檢索\n",
        "query = \"Describe the five roles in MetaGPT framework\"\n",
        "print(f\"正在執行相似性檢索: {query}\")\n",
        "similarity_context = retriever.retrieve(query)\n",
        "\n",
        "# 打印檢索到的相似內容\n",
        "print(\"\\n相似性上下文:\")\n",
        "for i, doc in enumerate(similarity_context, start=1):\n",
        "    print(f\"\\n--- Context {i} ---\")\n",
        "    print(doc.get_content())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8OtwZfUdszf",
        "outputId": "bfadf6db-c0a7-4da6-b225-5a4083042b8e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "正在執行相似性檢索: Describe the five roles in MetaGPT framework\n",
            "\n",
            "相似性上下文:\n",
            "\n",
            "--- Context 1 ---\n",
            "Preprint\n",
            "Table 2: Comparison of capabilities for MetaGPT and other approaches. ‘!’ indicates the\n",
            "presence of a specific feature in the corresponding framework, ‘%’ its absence.\n",
            "Framework Capabiliy AutoGPT LangChain AgentVerse ChatDev MetaGPT\n",
            "PRD generation % % % % !\n",
            "Tenical design genenration % % % % !\n",
            "API interface generation % % % % !\n",
            "Code generation ! ! ! ! !\n",
            "Precompilation execution % % % % !\n",
            "Role-based task management % % % ! !\n",
            "Code review % % ! ! !\n",
            "Table 3: Ablation study on roles. ‘#’ denotes ‘The number of’, ‘Product’ denotes ‘Product man-\n",
            "ager’, and ‘Project’ denotes ‘Project manager’. ‘ !’ indicates the addition of a specific role. ‘Revi-\n",
            "sions’ refers to ‘Human Revision Cost’.\n",
            "Engineer Product Architect Project #Agents #Lines Expense Revisions Executability\n",
            "! % % % 1 83.0 $ 0.915 10 1.0\n",
            "! ! % % 2 112.0 $ 1.059 6.5 2.0\n",
            "! ! ! % 3 143.0 $ 1.204 4.0 2.5\n",
            "! ! % ! 3 205.0 $ 1.251 3.5 2.0\n",
            "! ! ! ! 4 191.0 $ 1.385 2.5 4.0\n",
            "can easily integrate SOP-like designs to improve their performance, similar to injecting chain-of-\n",
            "thought (Wei et al., 2022) in LLMs.\n",
            "4.4 A BLATION STUDY\n",
            "The Effectiveness of Roles To understand the impact of different roles on the final results, we\n",
            "perform two tasks that involve generating effective code and calculating average statistics. When we\n",
            "exclude certain roles, unworkable codes are generated. As indicated by Table 3, the addition of roles\n",
            "different from just the Engineer consistently improves both revisions and executability. While more\n",
            "roles slightly increase the expenses, the overall performance improves noticeably, demonstrating the\n",
            "effectiveness of the various roles.\n",
            "The Effectiveness of Executable Feedback Mechanism As shown in Figure 4, adding executable\n",
            "feedback into MetaGPT leads to a significant improvement of 4.2% and 5.4% in Pass @1on Hu-\n",
            "manEval and MBPP, respectively. Besides, Table 1 shows that the feedback mechanism improves\n",
            "feasibility (3.67 to 3.75) and reduces the cost of human revisions (2.25 to 0.83). These results\n",
            "illustrate how our designed feedback mechanism can produce higher-quality code. Additional quan-\n",
            "titative results of MetaGPT and MetaGPT without executable feedback are shown in Table 4 and\n",
            "Table 9.\n",
            "5 C ONCLUSION\n",
            "This work introduces MetaGPT, a novel meta-programming framework that leverages SOPs to en-\n",
            "hance the problem-solving capabilities of multi-agent systems based on Large Language Models\n",
            "(LLMs). MetaGPT models a group of agents as a simulated software company, analogous to simu-\n",
            "lated towns (Park et al., 2023) and the Minecraft Sandbox in V oyager (Wang et al., 2023a). MetaGPT\n",
            "leverages role specialization, workflow management, and efficient sharing mechanisms such as mes-\n",
            "sage pools and subscriptions, rendering it a flexible and portable platform for autonomous agents\n",
            "and multi-agent frameworks. It uses an executable feedback mechanism to enhance code generation\n",
            "quality during runtime. In extensive experiments, MetaGPT achieves state-of-the-art performance\n",
            "on multiple benchmarks. The successful integration of human-like SOPs inspires future research\n",
            "on human-inspired techniques for artificial multi-agent systems. We also view our work as an early\n",
            "attempt to regulate LLM-based multi-agent frameworks. See also the outlook (Appendix A).\n",
            "9\n",
            "\n",
            "--- Context 2 ---\n",
            "Preprint\n",
            "Figure 2: An example of the communication protocol (left) and iterative programming with exe-\n",
            "cutable feedback (right). Left: Agents use a shared message pool to publish structured messages.\n",
            "They can also subscribe to relevant messages based on their profiles. Right: After generating the\n",
            "initial code, the Engineer agent runs and checks for errors. If errors occur, the agent checks past\n",
            "messages stored in memory and compares them with the PRD, system design, and code files.\n",
            "3 M ETAGPT: A M ETA-PROGRAMMING FRAMEWORK\n",
            "MetaGPT is a meta-programming framework for LLM-based multi-agent systems. Sec. 3.1 pro-\n",
            "vides an explanation of role specialization, workflow and structured communication in this frame-\n",
            "work, and illustrates how to organize a multi-agent system within the context of SOPs. Sec. 3.2\n",
            "presents a communication protocol that enhances role communication efficiency. We also imple-\n",
            "ment structured communication interfaces and an effective publish-subscribe mechanism. These\n",
            "methods enable agents to obtain directional information from other roles and public information\n",
            "from the environment. Finally, we introduce executable feedback—a self-correction mechanism for\n",
            "further enhancing code generation quality during run-time in Sec. 3.3.\n",
            "3.1 A GENTS IN STANDARD OPERATING PROCEDURES\n",
            "Specialization of Roles Unambiguous role specialization enables the breakdown of complex work\n",
            "into smaller and more specific tasks. Solving complex tasks or problems often requires the collab-\n",
            "oration of agents with diverse skills and expertise, each contributing specialized outputs tailored to\n",
            "specific issues.\n",
            "In a software company, a Product Manager typically conducts business-oriented analysis and derives\n",
            "insights, while a software engineer is responsible for programming. We define five roles in our\n",
            "software company: Product Manager, Architect, Project Manager, Engineer, and QA Engineer, as\n",
            "shown in Figure 1. In MetaGPT, we specify the agent’s profile, which includes their name, profile,\n",
            "goal, and constraints for each role. We also initialize the specific context and skills for each role.\n",
            "For instance, a Product Manager can use web search tools, while an Engineer can execute code, as\n",
            "shown in Figure 2. All agents adhere to the React-style behavior as described in Yao et al. (2022).\n",
            "Every agent monitors the environment ( i.e., the message pool in MetaGPT) to spot important ob-\n",
            "servations (e.g.,, messages from other agents). These messages can either directly trigger actions or\n",
            "assist in finishing the job.\n",
            "Workflow across Agents By defining the agents’ roles and operational skills, we can establish\n",
            "basic workflows. In our work, we follow SOP in software development, which enables all agents to\n",
            "work in a sequential manner.\n",
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 開啟查詢"
      ],
      "metadata": {
        "id": "rczdm5me2w_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"What are the five roles in the MetaGPT framework?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdwCdkPQ3Ype",
        "outputId": "fcad6770-06b6-4f7a-97a4-081a4e2f77d0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Product Manager, Architect, Project Manager, Engineer, QA Engineer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = query_engine.query(\"說明 MetaGPT 框架中的五個角色\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdRV2utQ3fCd",
        "outputId": "1102478b-081c-494d-b910-72f440c66699"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MetaGPT框架中的五個角色是產品經理（Product Manager）、架構師（Architect）、專案經理（Project Manager）、工程師（Engineer）和品質保證工程師（QA Engineer）。每個角色在框架中有特定的設定，包括他們的名稱、設定、目標和約束，並為每個角色初始化特定的上下文和技能。例如，產品經理可以使用網絡搜索工具，而工程師可以執行代碼。在MetaGPT中，所有代理都遵循React風格的行為。每個代理監控環境（即MetaGPT中的消息池）以發現重要的觀察結果，這些消息可以直接觸發操作或幫助完成工作。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TSV8TSCUCc1b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}